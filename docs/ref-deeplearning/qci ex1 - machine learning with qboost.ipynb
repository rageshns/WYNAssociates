{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "xPbokkfOMgTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to directly solving optimization, quadratic unconstrained binary optimization solvers like ours have machine learning applications. The specific application we demonstrate here is one known as boosting. Particularly we are demonstrating a variant of boosting that has been adapted to quadratic solvers known as QBoost. The underlying idea of boosting is to use many sources of imperfect information to build a strong prediction. In machine learning language, we find a collection of weak classifiers that together can form a strong classifier. A weighted combination of these sources of partial information can provide a powerful tool if combined in the right way. The task that the Dirac device will be doing is to find this combination. An obvious constraint is to include the classifiers that give the most accurate information, but there is another concern. We want ones that give complementary information. Statistically speaking, we want to take classifiers that have high correlations with the information that we want to classify, but have weak correlations between them. In the extreme case, two classifiers could give the exact same information, in which case including both is wasteful. However, avoiding waste isn't the only concern here. Including too many classifiers can also lead to overfitting if they capture spurious information specific to the training data, rather than information that will generalize well to unseen cases. In this tutorial, we show an implementation of QBoost and test it on a simple binary classification problem using the IRIS dataset."
      ],
      "metadata": {
        "id": "7s3YBih8MiOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importance\n",
        "\n",
        "An advantage of boosting is that once the strong classifier is built, it can be applied without having to re-solve the QUBO. As a result, the classifier can be applied in settings where access to Dirac is not available. As Dirac only gets used in the training phase, it also can be reused many times in the future. This simple application provides one example of many potential machine learning applications of our hardware.\n",
        "\n"
      ],
      "metadata": {
        "id": "6sNzjlJJMkQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application\n",
        "\n",
        "Classification, the task that QBoost performs, appears in a number of settings. A simple example of a classification problem that you are probably impacted by every day is email spam filtering. Here, the goal is to categorize email as \"spam\" or \"legitimate\", and it is relatively straightforward to see how the boosting approach can be applied. A variety of weak rules can be derived, (for example, a spam email is probably slightly more likely to contain the word \"money\"). These are of little use individually, but can be made into a powerful filter when combined through boosting. Disease diagnosis is also fundamentally a classification problem with a concrete example being the use of boosting to predict chronic kidney disease. The weak classifiers would come from patient medical history, such as whether they have other conditions or not, as well as other factors such as age. Also, boosting approaches can be applied to image recognition. This is done by using simple features (for example, a nose between two eyes represented by a lighter rectangle between two darker ones) as weak classifiers, and checking for combinations of them, as was done here for facial recognition."
      ],
      "metadata": {
        "id": "x9AnF55_MnPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methodology\n",
        "\n",
        "# Methodology\n",
        "\n",
        "The idea is based on the concept of boosting. Let us assume that we have a collection of $N$ *weak* classifiers $h_i$ where $i = 1, 2, \\ldots, N$. The goal is to construct a *strong* classifier as a linear superposition of these weak classifiers, that is:\n",
        "\n",
        "$$\n",
        "y = \\sum_{i=1}^N w_i \\, h_i(x)\n",
        "$$\n",
        "\n",
        "where $x$ is a vector of input features and $y \\in \\{-1, 1\\}$. The goal is to find $\\{w_i\\}$, the weights associated with the weak classifiers.\n",
        "\n",
        "Let us have a training set $\\{(x_s, y_s) \\mid s = 1, 2, \\ldots, S\\}$ of size $S$. We can determine optimal weights $w_i$ by minimizing:\n",
        "\n",
        "$$\n",
        "\\min_w \\sum_{s=1}^S \\Biggl[\\sum_{i=1}^N w_i \\, h_i(x_s) - y_s\\Biggr]^2\n",
        "\\;+\\; \\lambda \\sum_{i=1}^N (w_i)^0\n",
        "$$\n",
        "\n",
        "where the regularization term $\\lambda \\sum_{i=1}^N (w_i)^0$ penalizes non-zero weights; $\\lambda$ is the regularization coefficient. Re-arranging the above equation yields,\n",
        "\n",
        "$$\n",
        "\\min_w \\;\n",
        "\\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^N w_i \\, w_j \\sum_{s=1}^S h_i(x_s)\\,h_j(x_s)\n",
        "\\;-\\; \\frac{2}{N}\\sum_{i=1}^N \\sum_{s=1}^S y_s \\, h_i(x_s) \\, w_i\n",
        "\\;+\\; \\lambda \\sum_{i=1}^N (w_i)^0\n",
        "$$\n",
        "\n",
        "where we assume that each weight $w_i$ is an integer. Each weight can be constructed using $D$ qubits as\n",
        "\n",
        "$$\n",
        "w_i\n",
        "= \\sum_{d=0}^{D-1} 2^d\\,x_{i,d}\n",
        "$$\n",
        "\n",
        "where $x_{i,d}$ are binary variables. Navin et al. ([arXiv:0811.0416](https://arxiv.org/abs/0811.0416)) reported that using $D = 1$ yields similar or improved generalized errors compared to $D > 1$. The regularization term $\\lambda \\sum_{i=1}^N (w_i)^0$ only works when $D = 1$, that is, when the weights are binary. The corresponding QUBO is then\n",
        "\n",
        "$$\n",
        "\\min_x \\; x^T (Q + P)\\, x\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "Q_{ij} = \\frac{1}{N} \\sum_{s=1}^S h_i(x_s)\\,h_j(x_s)\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "P_{ij}\n",
        "= \\delta_{ij} \\Bigl(\\lambda\n",
        "  - \\frac{2}{N} \\sum_{s=1}^S h_i(x_s)\\,y_s \\Bigr).\n",
        "$$\n",
        "\n",
        "Note that the regularization term is designed to push many weights to zero, so only a subset of the weak classifiers is chosen. In the implementation that follows, we have used decision tree classifiers based on one, two, or three of the features as the weak classifiers.\n"
      ],
      "metadata": {
        "id": "vtNo3vK-MsJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade \"qci-client<5\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8cbfpMd73u2",
        "outputId": "1635d3e7-60f0-43e7-c779-556637f5aa92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qci-client<5 in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
            "Requirement already satisfied: requests<3,>=2.22.1 in /usr/local/lib/python3.10/dist-packages (from qci-client<5) (2.32.3)\n",
            "Requirement already satisfied: requests-futures<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from qci-client<5) (1.0.2)\n",
            "Requirement already satisfied: networkx<3,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from qci-client<5) (2.8.8)\n",
            "Requirement already satisfied: numpy<2,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from qci-client<5) (1.26.4)\n",
            "Requirement already satisfied: scipy<2,>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from qci-client<5) (1.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.22.1->qci-client<5) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.22.1->qci-client<5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.22.1->qci-client<5) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.22.1->qci-client<5) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation QBoost Algorithm\n",
        "\n",
        "We have implemented the QBoost algorithm that was explained above as a class in Python.\n",
        "\n"
      ],
      "metadata": {
        "id": "RrGltR6hTlAN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3Miujaw7Vd5"
      },
      "outputs": [],
      "source": [
        "from qci_client import QciClient\n",
        "token = \"xxx\"\n",
        "api_url = \"https://api.qci-prod.com\"\n",
        "qci = QciClient(api_token=token, url=api_url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libs\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "from functools import wraps\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import minimize\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        ")\n",
        "\n",
        "\n",
        "PLOT_FLAG = False\n",
        "\n",
        "\n",
        "def timer(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        beg_time = time.time()\n",
        "        val = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        tot_time = end_time - beg_time\n",
        "\n",
        "        print(\"Runtime of %s: %0.2f seconds!\" % (func.__name__, tot_time,))\n",
        "\n",
        "        return val\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "from typing import List\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "class WeakClassifierMLP:\n",
        "    \"\"\"\n",
        "    A simple wrapper that uses MLPClassifier on a chosen subset of features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, fea_ind_list: List[int], X_train: np.ndarray, y_train: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the weak classifier.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        fea_ind_list : List[int]\n",
        "            Indices of features to use for this classifier.\n",
        "        X_train : np.ndarray\n",
        "            Training data of shape (num_samples, num_features).\n",
        "        y_train : np.ndarray\n",
        "            Training labels of shape (num_samples,).\n",
        "        \"\"\"\n",
        "        # Validate the shapes\n",
        "        assert X_train.shape[0] == len(y_train), \\\n",
        "            \"X_train and y_train must have the same number of samples.\"\n",
        "\n",
        "        self.fea_ind_list = fea_ind_list\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "        # Instantiate a simple MLP\n",
        "        # Set small layer sizes to keep it \"weak\" (shallow or small hidden layer).\n",
        "        # You could tune random_state, hidden_layer_sizes, etc. as desired.\n",
        "        self.clf = MLPClassifier(\n",
        "            hidden_layer_sizes=(5,),  # just an example\n",
        "            max_iter=200,\n",
        "            random_state=0\n",
        "        )\n",
        "\n",
        "    def train(self) -> None:\n",
        "        \"\"\"\n",
        "        Fit the MLP model on the subset of features.\n",
        "        \"\"\"\n",
        "        # Slice X_train to keep only chosen features\n",
        "        X_subset = self.X_train[:, self.fea_ind_list]\n",
        "        # Fit the classifier\n",
        "        self.clf.fit(X_subset, self.y_train)\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict labels (+1/-1) for the given samples X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.ndarray\n",
        "            Input data of shape (num_samples, num_features).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            Array of predictions (+1 or -1) for each sample.\n",
        "        \"\"\"\n",
        "        # Keep only the chosen features\n",
        "        X_subset = X[:, self.fea_ind_list]\n",
        "\n",
        "        # Get the raw predictions (by default might be 0 or 1 if y_train was 0/1)\n",
        "        raw_preds = self.clf.predict(X_subset)\n",
        "\n",
        "        # Convert predictions to {-1, +1} if necessary\n",
        "        unique_labels = np.unique(self.y_train)\n",
        "        if set(unique_labels) == {0, 1}:\n",
        "            # Convert 0 -> -1\n",
        "            converted_preds = np.where(raw_preds == 0, -1, 1)\n",
        "        else:\n",
        "            # If already in {-1, +1}, no conversion needed\n",
        "            converted_preds = raw_preds\n",
        "\n",
        "        return converted_preds\n",
        "\n",
        "\n",
        "class QBoost:\n",
        "    def __init__(\n",
        "        self,\n",
        "        lambda_coef,\n",
        "        num_eqc_samples=10,\n",
        "        alpha=1.0,\n",
        "        theta=0.0,\n",
        "        mode=\"dct\",\n",
        "    ):\n",
        "\n",
        "        self.lambda_coef = lambda_coef\n",
        "        self.num_eqc_samples = num_eqc_samples\n",
        "        self.alpha = alpha\n",
        "        self.theta = theta\n",
        "        self.mode = mode\n",
        "        self.weights = None\n",
        "        self.h_list = None\n",
        "\n",
        "\n",
        "    @timer\n",
        "    def _build_weak_classifiers_dct(self, X: np.ndarray, y: np.ndarray):\n",
        "        S = X.shape[0]\n",
        "        M = X.shape[1]\n",
        "        assert len(y) == S\n",
        "\n",
        "        h_list = []\n",
        "\n",
        "        # Single-feature classifiers\n",
        "        for l in range(M):\n",
        "            weak_classifier = WeakClassifierMLP([l], X, y)\n",
        "            weak_classifier.train()\n",
        "            h_list.append(weak_classifier)\n",
        "\n",
        "        # Pairs of features\n",
        "        for i in range(M):\n",
        "            for j in range(i + 1, M):\n",
        "                weak_classifier = WeakClassifierMLP([i, j], X, y)\n",
        "                weak_classifier.train()\n",
        "                h_list.append(weak_classifier)\n",
        "\n",
        "        # Triplets of features\n",
        "        for i in range(M):\n",
        "            for j in range(i + 1, M):\n",
        "                for k in range(j + 1, M):\n",
        "                    weak_classifier = WeakClassifierMLP([i, j, k], X, y)\n",
        "                    weak_classifier.train()\n",
        "                    h_list.append(weak_classifier)\n",
        "\n",
        "        return h_list\n",
        "\n",
        "    @timer\n",
        "    def _get_hamiltonian(self, X, y):\n",
        "\n",
        "        S = X.shape[0]\n",
        "        M = X.shape[1]\n",
        "\n",
        "        if self.mode == \"dct\":\n",
        "            h_list = self._build_weak_classifiers_dct(X, y)\n",
        "            print('h_list', h_list)\n",
        "        else:\n",
        "            assert False, \"Incorrect mode <%s>!\" % self.mode\n",
        "\n",
        "        self.h_list = h_list\n",
        "\n",
        "        N = 14 # len(h_list)\n",
        "\n",
        "        Q = np.zeros(shape=(N, N), dtype=\"d\")\n",
        "        P = np.zeros(shape=(N, N), dtype=\"d\")\n",
        "\n",
        "        h_vals = np.array([h_list[i].predict(X) for i in range(N)])\n",
        "\n",
        "        assert h_vals.shape[0] == N\n",
        "        assert h_vals.shape[1] == S\n",
        "\n",
        "        for i in range(N):\n",
        "            P[i][i] = self.lambda_coef - (2.0 / N) * np.sum(h_vals[i] * y)\n",
        "            for j in range(N):\n",
        "                Q[i][j] = (1.0 / N ** 2) * np.sum(h_vals[i] * h_vals[j])\n",
        "\n",
        "        # Calculate the Hamiltonian\n",
        "        H = Q + P\n",
        "\n",
        "        # make sure H is symmetric up to machine precision\n",
        "        H = 0.5 * (H + H.transpose())\n",
        "\n",
        "        print(\"The size of the hamiltonian is %d by %d\" % (N, N))\n",
        "\n",
        "        return H\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.weights = weights\n",
        "\n",
        "    @timer\n",
        "    def train(self, X, y):\n",
        "\n",
        "        H = self._get_hamiltonian(X, y)\n",
        "\n",
        "        N = H.shape[0]\n",
        "\n",
        "        qubo_json = {\n",
        "            \"file_name\": \"qboost.json\",\n",
        "            \"file_config\": {\n",
        "                \"qubo\": {\"data\": H, \"num_variables\": N},\n",
        "            }\n",
        "        }\n",
        "\n",
        "        job_json = {\n",
        "            \"job_name\": \"qboost_classifier\",\n",
        "            \"job_tags\": [\"qboost\"],\n",
        "            \"params\": {\n",
        "                \"device_type\": \"eqc1\",\n",
        "                \"num_samples\": self.num_eqc_samples,\n",
        "                \"alpha\": self.alpha,\n",
        "            },\n",
        "        }\n",
        "\n",
        "        # Solve the optimization problem\n",
        "        #qci = QciClient()\n",
        "\n",
        "        response_json = qci.upload_file(file=qubo_json)\n",
        "        qubo_file_id = response_json[\"file_id\"]\n",
        "\n",
        "        # Setup job json\n",
        "        job_params = {\n",
        "            \"device_type\": \"dirac-1\",\n",
        "            \"alpha\": self.alpha,\n",
        "            \"num_samples\": self.num_eqc_samples,\n",
        "\n",
        "        }\n",
        "        job_json = qci.build_job_body(\n",
        "            job_type=\"sample-qubo\",\n",
        "            job_params=job_params,\n",
        "            qubo_file_id=qubo_file_id,\n",
        "            job_name=\"tutorial_eqc1\",\n",
        "            job_tags=[\"tutorial_eqc1\"],\n",
        "        )\n",
        "        print(job_json)\n",
        "\n",
        "        # Run the job\n",
        "        job_response_json = qci.process_job(\n",
        "            job_body=job_json,\n",
        "        )\n",
        "\n",
        "        print(job_response_json)\n",
        "\n",
        "        results = job_response_json[\"results\"]\n",
        "        energies = results[\"energies\"]\n",
        "        samples = results[\"solutions\"]\n",
        "\n",
        "        if True:\n",
        "            print(\"Energies:\", energies)\n",
        "\n",
        "        # The sample solutions are sorted by energy\n",
        "        sol = samples[0]\n",
        "\n",
        "        assert len(sol) == N, \"Inconsistent solution size!\"\n",
        "\n",
        "        self.weights = np.array(sol)\n",
        "\n",
        "        return\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        assert self.weights is not None, \"Model is not trained!\"\n",
        "        assert self.h_list is not None, \"Model is not trained!\"\n",
        "\n",
        "        assert len(self.weights) == len(self.h_list), \"Inconsisent sizes!\"\n",
        "\n",
        "        N = len(self.weights)\n",
        "        tmp_vals = np.zeros(shape=(X.shape[0]), dtype=\"d\")\n",
        "\n",
        "        fct = sum(self.weights)\n",
        "        if fct > 0:\n",
        "            fct = 1.0 / fct\n",
        "\n",
        "        for i in range(N):\n",
        "            tmp_vals += self.weights[i] * self.h_list[i].predict(X)\n",
        "\n",
        "        tmp_vals = fct * tmp_vals\n",
        "\n",
        "        pred_vals = np.sign(tmp_vals - self.theta)\n",
        "\n",
        "        for i in range(len(pred_vals)):\n",
        "            if pred_vals[i] == 0:\n",
        "                pred_vals[i] = -1.0\n",
        "\n",
        "        return pred_vals\n",
        "\n",
        "    def save_weights(self, file_name):\n",
        "        np.save(file_name, self.weights)"
      ],
      "metadata": {
        "id": "jbdzyLqd7_hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acquire Data: Iris\n",
        "\n",
        "The above class can then be used to build a classifier using the IRIS dataset. We have used 80% of the data for training and the rest is used for testing."
      ],
      "metadata": {
        "id": "TM0X6VjjTqO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Some parameters\n",
        "TEST_SIZE = 0.2\n",
        "LAMBDA_COEF = 1.0\n",
        "\n",
        "# Read dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "metadata": {
        "id": "vnwZ0QUQ8Gy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtUQHThx8QAo",
        "outputId": "3f80eb77-ecf8-482d-e9da-2c6d8a5d8b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150, 4)\n",
            "(150,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acquire Data: MNIST"
      ],
      "metadata": {
        "id": "ZC9TLJlxTsw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "FZYWgUDu8SRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('sample_data/mnist_train_small.csv', header=None)\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAa_UJnc8XwK",
        "outputId": "8ad3e283-d404-42e9-d593-99242f25f619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 785)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.loc[:, 1:]\n",
        "y = data.loc[:, 0]\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIQCVFaW8gkf",
        "outputId": "06659c20-46f6-4161-9e44-e352e8515b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 784)\n",
            "(20000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.to_numpy()\n",
        "y = y.to_numpy()\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjBms3hz-qY2",
        "outputId": "ef02df58-bafa-4887-ec9e-c8080fe7b2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 784)\n",
            "(20000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzbdSuH6NF_t",
        "outputId": "adc89b30-49a7-4195-9818-4b283086da00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 5, 7, ..., 2, 9, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(len(y)):\n",
        "    if y[i] == 0:\n",
        "        y[i] = -1\n",
        "    elif y[i] == 2:\n",
        "        y[i] = 1"
      ],
      "metadata": {
        "id": "G9ixrXgW8PBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m050_aumQbTE",
        "outputId": "3a4c4ffb-b967-448a-ff8a-b5b184065215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1,  1,  1, ...,  1,  1,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: Calling `QBoost` Model"
      ],
      "metadata": {
        "id": "o2gGgZ1IidoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=42,\n",
        ")\n",
        "\n",
        "obj = QBoost(lambda_coef=LAMBDA_COEF, num_eqc_samples=10, alpha=1.0, mode=\"dct\")\n",
        "\n",
        "obj.train(X_train, y_train)\n",
        "\n",
        "y_train_prd = obj.predict(X_train)\n",
        "y_test_prd = obj.predict(X_test)"
      ],
      "metadata": {
        "id": "zG8J4aiJQaLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference and Performance"
      ],
      "metadata": {
        "id": "rs85hM1digTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "print(\n",
        "    \"Train precision:\",\n",
        "    precision_score(y_train, y_train_prd, labels=[-1, 1], pos_label=1),\n",
        ")\n",
        "print(\n",
        "    \"Train recall:\",\n",
        "    recall_score(y_train, y_train_prd, labels=[-1, 1], pos_label=1),\n",
        ")\n",
        "print(\n",
        "    \"Train accuracy:\",\n",
        "    accuracy_score(y_train, y_train_prd),\n",
        ")\n",
        "\n",
        "sn.set(font_scale=1.4)\n",
        "train_conf_mat = confusion_matrix(y_train, y_train_prd, labels=[-1, 1])\n",
        "sn.heatmap(train_conf_mat, annot=True, annot_kws={\"size\": 16})\n",
        "plt.show()\n",
        "\n",
        "print(\n",
        "    \"Test precision:\",\n",
        "    precision_score(y_test, y_test_prd, labels=[-1, 1], pos_label=1),\n",
        ")\n",
        "print(\n",
        "    \"Test recall:\",\n",
        "    recall_score(y_test, y_test_prd, labels=[-1, 1], pos_label=1),\n",
        ")\n",
        "print(\n",
        "    \"Test accuracy:\",\n",
        "    accuracy_score(y_test, y_test_prd),\n",
        ")\n",
        "\n",
        "test_conf_mat = confusion_matrix(y_test, y_test_prd, labels=[-1, 1])\n",
        "sn.heatmap(test_conf_mat, annot=True, annot_kws={\"size\": 16})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TGN-Ivno---x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7fh1383xO162"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}