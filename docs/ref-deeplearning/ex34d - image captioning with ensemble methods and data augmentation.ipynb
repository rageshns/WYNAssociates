{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Glwyf8Kx1XaK"
      },
      "source": [
        "# Image Captioning\n",
        "\n",
        "**Author:** [A_K_Nain](https://twitter.com/A_K_Nain)<br>\n",
        "**Date created:** 2021/05/29<br>\n",
        "**Last modified:** 2021/10/31<br>\n",
        "**Description:** Implement an image captioning model using a CNN and a Transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vagE-NK41XaL"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CsJ7-sk81XaL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.applications import efficientnet\n",
        "from keras.layers import TextVectorization\n",
        "\n",
        "keras.utils.set_random_seed(111)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bbAytyV1XaM"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "We will be using the Flickr8K dataset for this tutorial. This dataset comprises over\n",
        "8,000 images, that are each paired with five different captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oLVZMTE11XaM"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "!unzip -qq Flickr8k_Dataset.zip\n",
        "!unzip -qq Flickr8k_text.zip\n",
        "!rm Flickr8k_Dataset.zip Flickr8k_text.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HB-hI1OO1XaM"
      },
      "outputs": [],
      "source": [
        "# Path to the images\n",
        "IMAGES_PATH = \"Flicker8k_Dataset\"\n",
        "\n",
        "# Desired image dimensions\n",
        "IMAGE_SIZE = (299, 299)\n",
        "\n",
        "# Vocabulary size\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "# Fixed length allowed for any sequence\n",
        "SEQ_LENGTH = 25\n",
        "\n",
        "# Dimension for the image embeddings and token embeddings\n",
        "EMBED_DIM = 512\n",
        "\n",
        "# Per-layer units in the feed-forward network\n",
        "FF_DIM = 512\n",
        "\n",
        "# Other training parameters\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJTJS0Sj1XaM"
      },
      "source": [
        "## Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OrPQhlo71XaN",
        "outputId": "e153fd2c-6633-4bdf-b9d8-5e2b8077147d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples:  6114\n",
            "Number of validation samples:  1529\n"
          ]
        }
      ],
      "source": [
        "def load_captions_data(filename):\n",
        "    \"\"\"Loads captions (text) data and maps them to corresponding images.\n",
        "\n",
        "    Args:\n",
        "        filename: Path to the text file containing caption data.\n",
        "\n",
        "    Returns:\n",
        "        caption_mapping: Dictionary mapping image names and the corresponding captions\n",
        "        text_data: List containing all the available captions\n",
        "    \"\"\"\n",
        "\n",
        "    with open(filename) as caption_file:\n",
        "        caption_data = caption_file.readlines()\n",
        "        caption_mapping = {}\n",
        "        text_data = []\n",
        "        images_to_skip = set()\n",
        "\n",
        "        for line in caption_data:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            # Image name and captions are separated using a tab\n",
        "            img_name, caption = line.split(\"\\t\")\n",
        "\n",
        "            # Each image is repeated five times for the five different captions.\n",
        "            # Each image name has a suffix `#(caption_number)`\n",
        "            img_name = img_name.split(\"#\")[0]\n",
        "            img_name = os.path.join(IMAGES_PATH, img_name.strip())\n",
        "\n",
        "            # We will remove caption that are either too short to too long\n",
        "            tokens = caption.strip().split()\n",
        "\n",
        "            if len(tokens) < 5 or len(tokens) > SEQ_LENGTH:\n",
        "                images_to_skip.add(img_name)\n",
        "                continue\n",
        "\n",
        "            if img_name.endswith(\"jpg\") and img_name not in images_to_skip:\n",
        "                # We will add a start and an end token to each caption\n",
        "                caption = \"<start> \" + caption.strip() + \" <end>\"\n",
        "                text_data.append(caption)\n",
        "\n",
        "                if img_name in caption_mapping:\n",
        "                    caption_mapping[img_name].append(caption)\n",
        "                else:\n",
        "                    caption_mapping[img_name] = [caption]\n",
        "\n",
        "        for img_name in images_to_skip:\n",
        "            if img_name in caption_mapping:\n",
        "                del caption_mapping[img_name]\n",
        "\n",
        "        return caption_mapping, text_data\n",
        "\n",
        "\n",
        "def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
        "    \"\"\"Split the captioning dataset into train and validation sets.\n",
        "\n",
        "    Args:\n",
        "        caption_data (dict): Dictionary containing the mapped caption data\n",
        "        train_size (float): Fraction of all the full dataset to use as training data\n",
        "        shuffle (bool): Whether to shuffle the dataset before splitting\n",
        "\n",
        "    Returns:\n",
        "        Traning and validation datasets as two separated dicts\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Get the list of all image names\n",
        "    all_images = list(caption_data.keys())\n",
        "\n",
        "    # 2. Shuffle if necessary\n",
        "    if shuffle:\n",
        "        np.random.shuffle(all_images)\n",
        "\n",
        "    # 3. Split into training and validation sets\n",
        "    train_size = int(len(caption_data) * train_size)\n",
        "\n",
        "    training_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
        "    }\n",
        "    validation_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
        "    }\n",
        "\n",
        "    # 4. Return the splits\n",
        "    return training_data, validation_data\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "captions_mapping, text_data = load_captions_data(\"Flickr8k.token.txt\")\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_data, valid_data = train_val_split(captions_mapping)\n",
        "print(\"Number of training samples: \", len(train_data))\n",
        "print(\"Number of validation samples: \", len(valid_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtlEFvHo1XaN"
      },
      "source": [
        "## Vectorizing the text data\n",
        "\n",
        "We'll use the `TextVectorization` layer to vectorize the text data,\n",
        "that is to say, to turn the\n",
        "original strings into integer sequences where each integer represents the index of\n",
        "a word in a vocabulary. We will use a custom string standardization scheme\n",
        "(strip punctuation characters except `<` and `>`) and the default\n",
        "splitting scheme (split on whitespace)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8aJcchMv1XaN"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
        "strip_chars = strip_chars.replace(\"<\", \"\")\n",
        "strip_chars = strip_chars.replace(\">\", \"\")\n",
        "\n",
        "vectorization = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LENGTH,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "vectorization.adapt(text_data)\n",
        "\n",
        "# Data augmentation for image data\n",
        "image_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.2),\n",
        "        layers.RandomContrast(0.3),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rynJsq-1XaN"
      },
      "source": [
        "## Building a `tf.data.Dataset` pipeline for training\n",
        "\n",
        "We will generate pairs of images and corresponding captions using a `tf.data.Dataset` object.\n",
        "The pipeline consists of two steps:\n",
        "\n",
        "1. Read the image from the disk\n",
        "2. Tokenize all the five captions corresponding to the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kiROGoiT1XaN"
      },
      "outputs": [],
      "source": [
        "def decode_and_resize(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img\n",
        "\n",
        "\n",
        "def process_input(img_path, captions):\n",
        "    return decode_and_resize(img_path), vectorization(captions)\n",
        "\n",
        "\n",
        "def make_dataset(images, captions):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n",
        "    dataset = dataset.shuffle(BATCH_SIZE * 8)\n",
        "    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Pass the list of images and the list of corresponding captions\n",
        "train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n",
        "\n",
        "valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNXbAFFZ1XaO"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Our image captioning architecture consists of three models:\n",
        "\n",
        "1. A CNN: used to extract the image features\n",
        "2. A TransformerEncoder: The extracted image features are then passed to a Transformer\n",
        "                    based encoder that generates a new representation of the inputs\n",
        "3. A TransformerDecoder: This model takes the encoder output and the text data\n",
        "                    (sequences) as inputs and tries to learn to generate the caption."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the model (advanced emsemble)"
      ],
      "metadata": {
        "id": "6sIzjq4fBENg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import (\n",
        "    VGG16,\n",
        "    ResNet50,\n",
        "    MobileNetV2,\n",
        "    InceptionV3,\n",
        "    EfficientNetB0,\n",
        ")\n",
        "\n",
        "############################################################\n",
        "# Updated CNN model function using multiple base backbones #\n",
        "############################################################\n",
        "def get_cnn_model() -> keras.Model:\n",
        "    \"\"\"\n",
        "    Creates a CNN feature-extraction model using five different base backbones\n",
        "    (VGG16, ResNet50, MobileNetV2, InceptionV3, and EfficientNetB0).\n",
        "    The outputs from these models are concatenated to form a unified image\n",
        "    embedding.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: A model that takes an image as input and outputs a combined\n",
        "                     feature representation.\n",
        "    \"\"\"\n",
        "    # Create a common input layer for all base models\n",
        "    input_tensor = layers.Input(shape=(*IMAGE_SIZE, 3), name=\"image_input\")\n",
        "\n",
        "    ###################################################################\n",
        "    # 1. Create each base model (with pretrained weights, no top) and #\n",
        "    #    freeze them so they don't get updated during training        #\n",
        "    ###################################################################\n",
        "    # VGG16\n",
        "    vgg16_base = VGG16(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        input_tensor=input_tensor,\n",
        "    )\n",
        "    for layer in vgg16_base.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # ResNet50\n",
        "    resnet_base = ResNet50(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        input_tensor=input_tensor,\n",
        "    )\n",
        "    for layer in resnet_base.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # MobileNetV2\n",
        "    mobilenet_base = MobileNetV2(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        input_tensor=input_tensor,\n",
        "    )\n",
        "    for layer in mobilenet_base.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # InceptionV3\n",
        "    inception_base = InceptionV3(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        input_tensor=input_tensor,\n",
        "    )\n",
        "    for layer in inception_base.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # EfficientNetB0\n",
        "    efficientnet_base = EfficientNetB0(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        input_tensor=input_tensor,\n",
        "    )\n",
        "    for layer in efficientnet_base.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    ######################################################\n",
        "    # 2. Concatenate all features along the last dimension\n",
        "    ######################################################\n",
        "    # We will globally average pool each model's output to get a (batch, features) shape,\n",
        "    # then expand dims to (batch, 1, features) and concatenate them.\n",
        "\n",
        "    vgg16_pooled = layers.GlobalAveragePooling2D()(vgg16_base.output)\n",
        "    vgg16_pooled = layers.Reshape((1, -1))(vgg16_pooled)\n",
        "\n",
        "    resnet_pooled = layers.GlobalAveragePooling2D()(resnet_base.output)\n",
        "    resnet_pooled = layers.Reshape((1, -1))(resnet_pooled)\n",
        "\n",
        "    mobilenet_pooled = layers.GlobalAveragePooling2D()(mobilenet_base.output)\n",
        "    mobilenet_pooled = layers.Reshape((1, -1))(mobilenet_pooled)\n",
        "\n",
        "    inception_pooled = layers.GlobalAveragePooling2D()(inception_base.output)\n",
        "    inception_pooled = layers.Reshape((1, -1))(inception_pooled)\n",
        "\n",
        "    efficientnet_pooled = layers.GlobalAveragePooling2D()(efficientnet_base.output)\n",
        "    efficientnet_pooled = layers.Reshape((1, -1))(efficientnet_pooled)\n",
        "\n",
        "    concatenated_out = layers.Concatenate(axis=-1)(\n",
        "        [\n",
        "            vgg16_pooled,\n",
        "            resnet_pooled,\n",
        "            mobilenet_pooled,\n",
        "            inception_pooled,\n",
        "            efficientnet_pooled,\n",
        "        ]\n",
        "    )\n",
        "    # Shape is (batch, 1, sum_of_features)\n",
        "\n",
        "    #########################################################################\n",
        "    # 3. Create a Model that outputs the concatenated embeddings            #\n",
        "    #########################################################################\n",
        "    cnn_model = keras.Model(inputs=input_tensor, outputs=concatenated_out)\n",
        "    return cnn_model\n",
        "\n",
        "\n",
        "##########################################\n",
        "#          Transformer components        #\n",
        "##########################################\n",
        "class TransformerEncoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim: int, dense_dim: int, num_heads: int, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n",
        "\n",
        "    def call(self, inputs: tf.Tensor, training: bool, mask: tf.Tensor = None) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the Transformer encoder block.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): The input tensor with shape (batch, sequence, features).\n",
        "            training (bool): Whether the model is in training mode.\n",
        "            mask (tf.Tensor, optional): A boolean mask for padding. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Output tensor of the same shape as inputs.\n",
        "        \"\"\"\n",
        "        # Normalize and feed to a dense layer\n",
        "        inputs = self.layernorm_1(inputs)\n",
        "        inputs = self.dense_1(inputs)\n",
        "\n",
        "        # Self-attention\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=None,\n",
        "            training=training,\n",
        "        )\n",
        "        # Add & norm\n",
        "        out_1 = self.layernorm_2(inputs + attention_output_1)\n",
        "        return out_1\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length: int, vocab_size: int, embed_dim: int, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
        "\n",
        "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for positional embeddings.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): Token IDs with shape (batch, sequence_length).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Token embeddings + positional embeddings\n",
        "        \"\"\"\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_tokens = embedded_tokens * self.embed_scale\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs: tf.Tensor, mask=None) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Compute a mask for padding tokens (assumed to be '0').\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): The input token IDs.\n",
        "            mask: Unused.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Boolean mask where False means padding (token=0).\n",
        "        \"\"\"\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim: int, ff_dim: int, num_heads: int, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n",
        "        self.ffn_layer_2 = layers.Dense(embed_dim)\n",
        "\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "\n",
        "        self.embedding = PositionalEmbedding(\n",
        "            embed_dim=EMBED_DIM,\n",
        "            sequence_length=SEQ_LENGTH,\n",
        "            vocab_size=VOCAB_SIZE,\n",
        "        )\n",
        "        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
        "\n",
        "        self.dropout_1 = layers.Dropout(0.3)\n",
        "        self.dropout_2 = layers.Dropout(0.5)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(\n",
        "        self, inputs: tf.Tensor, encoder_outputs: tf.Tensor, training: bool, mask: tf.Tensor = None\n",
        "    ) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the Transformer decoder block.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): The token IDs for the decoder, shape (batch, seq_length).\n",
        "            encoder_outputs (tf.Tensor): Outputs from the encoder, shape (batch, 1, features).\n",
        "            training (bool): Whether the model is in training mode.\n",
        "            mask (tf.Tensor, optional): Boolean mask. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Logits over VOCAB_SIZE, shape (batch, seq_length, vocab_size).\n",
        "        \"\"\"\n",
        "        # Token + positional embeddings\n",
        "        inputs = self.embedding(inputs)\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
        "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
        "        else:\n",
        "            combined_mask = causal_mask\n",
        "            padding_mask = None\n",
        "\n",
        "        # Decoder self-attention\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=combined_mask,\n",
        "            training=training,\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        # Cross-attention with encoder outputs\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "            training=training,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_out = self.ffn_layer_1(out_2)\n",
        "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
        "        ffn_out = self.ffn_layer_2(ffn_out)\n",
        "\n",
        "        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n",
        "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
        "        preds = self.out(ffn_out)\n",
        "        return preds\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Builds a causal (future-masking) attention mask to use in the\n",
        "        self-attention mechanism.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): The input tensor, shape (batch, seq_length, features).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: A causal mask of shape (batch, seq_length, seq_length).\n",
        "        \"\"\"\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, sequence_length, sequence_length))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "#####################################################\n",
        "#       The main Image Captioning model class       #\n",
        "#####################################################\n",
        "class ImageCaptioningModel(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cnn_model: keras.Model,\n",
        "        encoder: TransformerEncoderBlock,\n",
        "        decoder: TransformerDecoderBlock,\n",
        "        num_captions_per_image: int = 5,\n",
        "        image_aug: tf.keras.Sequential = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        A custom model to tie together the CNN-based feature extractor,\n",
        "        Transformer encoder, and Transformer decoder.\n",
        "\n",
        "        Args:\n",
        "            cnn_model (keras.Model): Pretrained CNN feature extractor.\n",
        "            encoder (TransformerEncoderBlock): Transformer encoder.\n",
        "            decoder (TransformerDecoderBlock): Transformer decoder.\n",
        "            num_captions_per_image (int, optional): Number of captions per image. Defaults to 5.\n",
        "            image_aug (tf.keras.Sequential, optional): Optional image augmentation pipeline. Defaults to None.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
        "        self.num_captions_per_image = num_captions_per_image\n",
        "        self.image_aug = image_aug\n",
        "\n",
        "    def apply_augmentation(self, images: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Applies random data augmentation transformations (e.g., flips, rotations, zoom)\n",
        "        to the input images. The captions remain unchanged.\n",
        "\n",
        "        Args:\n",
        "            images (tf.Tensor): A batch of images of shape (batch, height, width, channels).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: A batch of augmented images, same shape as input.\n",
        "        \"\"\"\n",
        "        if self.image_aug:\n",
        "            images = self.image_aug(images)\n",
        "        return images\n",
        "\n",
        "    def calculate_loss(\n",
        "        self, y_true: tf.Tensor, y_pred: tf.Tensor, mask: tf.Tensor\n",
        "    ) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Applies the model's loss function, taking the mask into account.\n",
        "\n",
        "        Args:\n",
        "            y_true (tf.Tensor): Ground-truth token IDs, shape (batch, seq_length).\n",
        "            y_pred (tf.Tensor): Predictions (logits), shape (batch, seq_length, vocab_size).\n",
        "            mask (tf.Tensor): Boolean mask for non-padding tokens, shape (batch, seq_length).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Scalar loss value.\n",
        "        \"\"\"\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "    def calculate_accuracy(\n",
        "        self, y_true: tf.Tensor, y_pred: tf.Tensor, mask: tf.Tensor\n",
        "    ) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Computes token-level accuracy under a given mask.\n",
        "\n",
        "        Args:\n",
        "            y_true (tf.Tensor): Ground-truth token IDs, shape (batch, seq_length).\n",
        "            y_pred (tf.Tensor): Predictions (logits), shape (batch, seq_length, vocab_size).\n",
        "            mask (tf.Tensor): Boolean mask for non-padding tokens, shape (batch, seq_length).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Scalar accuracy value.\n",
        "        \"\"\"\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
        "        accuracy = tf.math.logical_and(mask, accuracy)\n",
        "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        "\n",
        "    def _compute_caption_loss_and_acc(\n",
        "        self, img_embed: tf.Tensor, batch_seq: tf.Tensor, training: bool = True\n",
        "    ) -> tuple[tf.Tensor, tf.Tensor]:\n",
        "        \"\"\"\n",
        "        Given a batch of image embeddings and corresponding caption sequences,\n",
        "        computes the loss and accuracy for one forward pass.\n",
        "\n",
        "        Args:\n",
        "            img_embed (tf.Tensor): Image embeddings from the CNN, shape (batch, 1, features).\n",
        "            batch_seq (tf.Tensor): Caption token IDs, shape (batch, seq_length).\n",
        "            training (bool): Whether we are in training mode.\n",
        "\n",
        "        Returns:\n",
        "            tuple[tf.Tensor, tf.Tensor]: Loss and accuracy for the given batch.\n",
        "        \"\"\"\n",
        "        encoder_out = self.encoder(img_embed, training=training)\n",
        "        batch_seq_inp = batch_seq[:, :-1]  # all but last token\n",
        "        batch_seq_true = batch_seq[:, 1:]  # all but first token\n",
        "        mask = tf.math.not_equal(batch_seq_true, 0)\n",
        "\n",
        "        batch_seq_pred = self.decoder(\n",
        "            batch_seq_inp, encoder_out, training=training, mask=mask\n",
        "        )\n",
        "        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
        "        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
        "        return loss, acc\n",
        "\n",
        "    def train_step(self, batch_data: tuple[tf.Tensor, tf.Tensor]) -> dict:\n",
        "        \"\"\"\n",
        "        Defines the forward + backward pass under the Keras .fit() loop.\n",
        "\n",
        "        Args:\n",
        "            batch_data (tuple[tf.Tensor, tf.Tensor]): A tuple of (images, token_sequences),\n",
        "                where 'images' shape = (batch_size, height, width, channels)\n",
        "                and 'token_sequences' shape = (batch_size, num_captions_per_image, seq_length).\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary of {'loss': ..., 'acc': ...} metrics.\n",
        "        \"\"\"\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss = 0.0\n",
        "        batch_acc = 0.0\n",
        "\n",
        "        # 1. Augment images if available (the captions remain the same)\n",
        "        batch_img = self.apply_augmentation(batch_img)\n",
        "\n",
        "        # 2. Get image embeddings\n",
        "        img_embed = self.cnn_model(batch_img)\n",
        "\n",
        "        # 3. Pass each of the 'num_captions_per_image' captions\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            with tf.GradientTape() as tape:\n",
        "                loss, acc = self._compute_caption_loss_and_acc(\n",
        "                    img_embed, batch_seq[:, i, :], training=True\n",
        "                )\n",
        "            batch_loss += loss\n",
        "            batch_acc += acc\n",
        "\n",
        "            # Update only encoder & decoder trainable variables\n",
        "            train_vars = (\n",
        "                self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "            )\n",
        "            grads = tape.gradient(loss, train_vars)\n",
        "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
        "\n",
        "        # Average the accuracy over the number of captions\n",
        "        batch_acc /= float(self.num_captions_per_image)\n",
        "\n",
        "        self.loss_tracker.update_state(batch_loss)\n",
        "        self.acc_tracker.update_state(batch_acc)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    def test_step(self, batch_data: tuple[tf.Tensor, tf.Tensor]) -> dict:\n",
        "        \"\"\"\n",
        "        Defines the forward pass for validation/testing under the Keras .fit() loop.\n",
        "\n",
        "        Args:\n",
        "            batch_data (tuple[tf.Tensor, tf.Tensor]): A tuple of (images, token_sequences).\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary of {'loss': ..., 'acc': ...} metrics.\n",
        "        \"\"\"\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss = 0.0\n",
        "        batch_acc = 0.0\n",
        "\n",
        "        # 1. Get image embeddings\n",
        "        img_embed = self.cnn_model(batch_img)\n",
        "\n",
        "        # 2. Pass each of the 'num_captions_per_image' captions\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            loss, acc = self._compute_caption_loss_and_acc(\n",
        "                img_embed, batch_seq[:, i, :], training=False\n",
        "            )\n",
        "            batch_loss += loss\n",
        "            batch_acc += acc\n",
        "\n",
        "        # Average the accuracy\n",
        "        batch_acc /= float(self.num_captions_per_image)\n",
        "\n",
        "        self.loss_tracker.update_state(batch_loss)\n",
        "        self.acc_tracker.update_state(batch_acc)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self) -> list:\n",
        "        \"\"\"\n",
        "        Lists the metrics so that reset_states() can be called automatically\n",
        "        at the start of each epoch in model.fit().\n",
        "\n",
        "        Returns:\n",
        "            list: A list containing the loss and accuracy trackers.\n",
        "        \"\"\"\n",
        "        return [self.loss_tracker, self.acc_tracker]\n",
        "\n",
        "\n",
        "###############################################\n",
        "# Instantiate your updated image caption model\n",
        "###############################################\n",
        "# 1. Create the multi-backbone CNN model\n",
        "cnn_model = get_cnn_model()\n",
        "\n",
        "# 2. Create transformer encoder & decoder\n",
        "encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1)\n",
        "decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2)\n",
        "\n",
        "# 3. (Optional) image augmentation, if you want to do random flips, rotations, etc.\n",
        "image_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.1),\n",
        "    ],\n",
        "    name=\"image_augmentation\",\n",
        ")\n",
        "\n",
        "# 4. Build the final image captioning model with data augmentation\n",
        "caption_model = ImageCaptioningModel(\n",
        "    cnn_model=cnn_model,\n",
        "    encoder=encoder,\n",
        "    decoder=decoder,\n",
        "    image_aug=image_augmentation,\n",
        ")"
      ],
      "metadata": {
        "id": "z1PABhb--_w_",
        "outputId": "ef0e72d3-487c-405e-92ae-dfd51b07e4c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-17dcc0ead514>:52: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  mobilenet_base = MobileNetV2(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjZ0zDgg1XaO"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "123-9hpu1XaP",
        "outputId": "07545cdc-0647-4eed-c877-aa4bfefffe97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 1, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 2, None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 3s/step - acc: 0.1797 - loss: 31.2147 - val_acc: 0.3352 - val_loss: 18.1499\n",
            "Epoch 2/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 3s/step - acc: 0.3394 - loss: 17.9645 - val_acc: 0.3491 - val_loss: 17.0664\n",
            "Epoch 3/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - acc: 0.3605 - loss: 16.4004"
          ]
        }
      ],
      "source": [
        "# Define the loss function\n",
        "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False,\n",
        "    reduction=None,\n",
        ")\n",
        "\n",
        "# EarlyStopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=50, restore_best_weights=True)\n",
        "\n",
        "\n",
        "# Learning Rate Scheduler for the optimizer\n",
        "class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, post_warmup_learning_rate, warmup_steps):\n",
        "        super().__init__()\n",
        "        self.post_warmup_learning_rate = post_warmup_learning_rate\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        global_step = tf.cast(step, tf.float32)\n",
        "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
        "        warmup_progress = global_step / warmup_steps\n",
        "        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n",
        "        return tf.cond(\n",
        "            global_step < warmup_steps,\n",
        "            lambda: warmup_learning_rate,\n",
        "            lambda: self.post_warmup_learning_rate,\n",
        "        )\n",
        "\n",
        "\n",
        "# Create a learning rate schedule\n",
        "num_train_steps = len(train_dataset) * EPOCHS\n",
        "num_warmup_steps = num_train_steps // 15\n",
        "lr_schedule = LRSchedule(post_warmup_learning_rate=1e-3, warmup_steps=num_warmup_steps)\n",
        "\n",
        "# Compile the model\n",
        "caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)\n",
        "\n",
        "# Fit the model\n",
        "caption_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_dataset,\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpLt86UM1XaP"
      },
      "source": [
        "## Check sample predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVLJhI1K1XaP"
      },
      "outputs": [],
      "source": [
        "vocab = vectorization.get_vocabulary()\n",
        "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
        "max_decoded_sentence_length = SEQ_LENGTH - 1\n",
        "valid_images = list(valid_data.keys())\n",
        "\n",
        "\n",
        "def generate_caption():\n",
        "    # Select a random image from the validation dataset\n",
        "    sample_img = np.random.choice(valid_images)\n",
        "\n",
        "    # Read the image from the disk\n",
        "    sample_img = decode_and_resize(sample_img)\n",
        "    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "    # Pass the image to the CNN\n",
        "    img = tf.expand_dims(sample_img, 0)\n",
        "    img = caption_model.cnn_model(img)\n",
        "\n",
        "    # Pass the image features to the Transformer encoder\n",
        "    encoded_img = caption_model.encoder(img, training=False)\n",
        "\n",
        "    # Generate the caption using the Transformer decoder\n",
        "    decoded_caption = \"<start> \"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n",
        "        mask = tf.math.not_equal(tokenized_caption, 0)\n",
        "        predictions = caption_model.decoder(\n",
        "            tokenized_caption, encoded_img, training=False, mask=mask\n",
        "        )\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = index_lookup[sampled_token_index]\n",
        "        if sampled_token == \"<end>\":\n",
        "            break\n",
        "        decoded_caption += \" \" + sampled_token\n",
        "\n",
        "    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n",
        "    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n",
        "    print(\"Predicted Caption: \", decoded_caption)\n",
        "\n",
        "\n",
        "# Check predictions for a few samples\n",
        "generate_caption()\n",
        "generate_caption()\n",
        "generate_caption()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nQW2G2u1XaP"
      },
      "source": [
        "## End Notes\n",
        "\n",
        "We saw that the model starts to generate reasonable captions after a few epochs. To keep\n",
        "this example easily runnable, we have trained it with a few constraints, like a minimal\n",
        "number of attention heads. To improve the predictions, you can try changing these training\n",
        "settings and find a good model for your use case."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}