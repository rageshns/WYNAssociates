{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Blog: From Sagemaker Endpoint to AWS Lambda and API Gateway\n",
        "\n",
        "This notebook walks you through the key steps of the following:\n",
        "\n",
        "1. Use Sagemaker to stand up an endpoint, [source](https://plainenglish.io/blog/how-to-use-llama-2-with-an-api-on-aws-to-power-your-ai-apps#step-1-go-to-aws-sagemaker)\n",
        "2. Create a Lambda function (see code below)\n",
        "3. Stand up an API for the Lambda"
      ],
      "metadata": {
        "id": "aGuWXnQs3FvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Payload"
      ],
      "metadata": {
        "id": "_mlgcMoJ1HHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The payload looks like the following:\n",
        "\n",
        "```json\n",
        "\n",
        "{\n",
        "  \"body\": {\n",
        "    \"inputs\": \"<s>[INST] what is the recipe of mayonnaise? [/INST] \",\n",
        "    \"parameters\": {\n",
        "      \"max_new_tokens\": 256,\n",
        "      \"top_p\": 0.9,\n",
        "      \"temperature\": 0.6\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "This is the `Event JSON` input. You can directly copy/paste the above into the `Event JSON` section of your lambda function."
      ],
      "metadata": {
        "id": "6M6XEc7w1H75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploying a Model on AWS Sagemaker\n",
        "\n",
        "### Prerequisites\n",
        "Before you begin, ensure you have a model endpoint already deployed on AWS Sagemaker. If not, follow the instructions below to set up everything you need from scratch.\n",
        "\n",
        "#### Step 1: Access AWS Sagemaker\n",
        "1. Log in to your AWS Management Console.\n",
        "2. Use the search bar at the top to find AWS Sagemaker.\n",
        "3. Select AWS Sagemaker to enter its dashboard.\n",
        "\n",
        "#### Step 2: Set up a Domain on AWS Sagemaker\n",
        "1. In the AWS Sagemaker dashboard, locate and click on \"Domains\" in the left sidebar.\n",
        "2. Choose \"Create a Domain\".\n",
        "3. Make sure the \"Quick Setup\" option is selected.\n",
        "4. Fill out the form:\n",
        "   - Enter a domain name of your choice.\n",
        "   - Configure the remaining settings as suggested or based on the provided screenshot.\n",
        "   - For newcomers, select \"Create a new role\" in the Execution role section. Experienced users can select a previously created role.\n",
        "5. Click \"Submit\" to create your domain.\n",
        "6. Record the username displayed on the screen; this is crucial for deploying your model.\n",
        "\n",
        "##### Troubleshooting\n",
        "If you encounter issues during domain creation, such as failures related to user permissions or VPC configuration, follow the suggested troubleshooting steps or consult the AWS documentation.\n",
        "\n",
        "#### Step 3: Start a Sagemaker Studio Session\n",
        "1. After your domain has been set up, click on the \"Studio\" link in the left sidebar.\n",
        "2. Select the domain and user profile you configured earlier.\n",
        "3. Click \"Open Studio\" to launch the session.\n",
        "\n",
        "#### Step 4: Select and Deploy the Llama-2-7b-chat Model\n",
        "1. Within Sagemaker Studio, navigate to \"Models, notebooks, and solutions\" under the SageMaker Jumpstart tab.\n",
        "2. Use the search bar to locate the Llama 2 model, specifically the 7b chat model.\n",
        "3. Click on the model to access its detailed page.\n",
        "4. Here, you can adjust deployment settings if necessary. However, for simplicity, proceed with the default Sagemaker settings.\n",
        "5. Deploy the model as configured. Note: The 70B version of this model requires a robust server. If you encounter deployment issues due to server constraints, consider submitting a request to AWS service quotas.\n",
        "6. Allow 5-10 minutes for the deployment process to complete. Once done, a confirmation screen will appear.\n",
        "7. Document the model's Endpoint name for future API interactions."
      ],
      "metadata": {
        "id": "jZvLHVJX2GXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lambda"
      ],
      "metadata": {
        "id": "DmKDFxY10z07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lambda function is defined in the following:\n",
        "\n",
        "```py\n",
        "import os\n",
        "import io\n",
        "import boto3\n",
        "import json\n",
        "\n",
        "# Grab environment variables\n",
        "ENDPOINT_NAME = os.environ['ENDPOINT_NAME']  # Get the SageMaker endpoint name from environment variables\n",
        "print(ENDPOINT_NAME)\n",
        "\n",
        "runtime = boto3.client('runtime.sagemaker')  # Create a SageMaker runtime client\n",
        "\n",
        "\n",
        "def lambda_handler(event: dict, context: object) -> dict:\n",
        "    \"\"\"\n",
        "    Lambda function handler that invokes a SageMaker endpoint.\n",
        "\n",
        "    Args:\n",
        "        event (dict): The input event data\n",
        "        context (object): The Lambda function context\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with the response status code and body\n",
        "    \"\"\"\n",
        "    # Invoke the SageMaker endpoint\n",
        "    response = runtime.invoke_endpoint(\n",
        "        EndpointName=ENDPOINT_NAME,  # Use the environment variable for the endpoint name\n",
        "        ContentType='application/json',  # Specify the content type as JSON\n",
        "        Body=json.dumps(event['body']),  # Pass the input data from the event\n",
        "        CustomAttributes=\"accept_eula=true\",  # Accept the EULA\n",
        "        InferenceComponentName=\"meta-textgeneration-llama-2-7b-f-20240509-223751\"\n",
        "    )\n",
        "\n",
        "    # Parse the response as JSON\n",
        "    result = json.loads(response['Body'].read().decode())\n",
        "\n",
        "    # Return a response with a 200 status code and the result as the body\n",
        "    return {\n",
        "        \"statusCode\": 200,\n",
        "        \"body\": json.dumps(result)\n",
        "    }\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "GyEUwaAQ01hv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python API Call via `requests`"
      ],
      "metadata": {
        "id": "a5q9DaGv2oiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from typing import Dict\n",
        "\n",
        "def call_llama(prompt: str, max_new_tokens: int = 50, temperature: float = 0.9) -> str:\n",
        "    \"\"\"\n",
        "    Calls the Llama API to generate text based on a given prompt, controlling the length and randomness.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The prompt text to send to the Llama model for text generation.\n",
        "        max_new_tokens (int, optional): The maximum number of tokens that the model should generate. Defaults to 50.\n",
        "        temperature (float, optional): Controls the randomness of the output. Lower values make the model more deterministic.\n",
        "            A higher value increases randomness. Defaults to 0.9.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text response from the Llama model.\n",
        "\n",
        "    Raises:\n",
        "        Exception: If the API call fails and returns a non-200 status code, it raises an exception with the error details.\n",
        "    \"\"\"\n",
        "    # API endpoint for the Llama model\n",
        "    api_url = \"https://v6rkdcyir7.execute-api.us-east-1.amazonaws.com/beta\"\n",
        "\n",
        "    # Configuration for the request body\n",
        "    json_body = {\n",
        "        \"body\": {\n",
        "            \"inputs\": f\"<s>[INST] {prompt} [/INST]\",\n",
        "            \"parameters\": {\n",
        "                \"max_new_tokens\": max_new_tokens,\n",
        "                \"top_p\": 0.9,  # Fixed probability cutoff to select tokens with cumulative probability above this threshold\n",
        "                \"temperature\": temperature\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Headers to indicate that the payload is JSON\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "    # Perform the POST request to the Llama API\n",
        "    response = requests.post(api_url, headers=headers, json=json_body)\n",
        "\n",
        "    # Parse the JSON response\n",
        "    response_body = response.json()['body']\n",
        "\n",
        "    # Convert the string response to a JSON object\n",
        "    body_list = json.loads(response_body)\n",
        "\n",
        "    # Extract the 'generated_text' from the first item in the list\n",
        "    generated_text = body_list[0]['generated_text']\n",
        "\n",
        "    # Separate the answer from the instruction\n",
        "    answer = generated_text.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    # Check the status code of the response\n",
        "    if response.status_code == 200:\n",
        "        return answer  # Return the text generated by the model\n",
        "    else:\n",
        "        # Raise an exception if the API did not succeed\n",
        "        raise Exception(f\"Error calling Llama API: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "4rA_xTAPiejP"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Example usage\n",
        "prompt = \"tell me a joke\"\n",
        "response = call_llama(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yji5K0qkight",
        "outputId": "ee2e4a70-bca4-4c8c-e0c7-4f361b9d08b9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here's a classic one:\n",
            "\n",
            "Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "I hope that made you smile! Do you want to hear another one?\n",
            "CPU times: user 88.5 ms, sys: 5.01 ms, total: 93.5 ms\n",
            "Wall time: 2.49 s\n"
          ]
        }
      ]
    }
  ]
}