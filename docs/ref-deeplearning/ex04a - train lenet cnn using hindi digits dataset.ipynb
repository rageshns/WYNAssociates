{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "pOb0hfsymooz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jG3TdnxDkOd2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle as pkl\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Path of Data"
      ],
      "metadata": {
        "id": "T_QMLX-5mshX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data folder\n",
        "DATA_FOLDER = \"./data\"\n",
        "hindi_handwritten_dataset_zip_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00389/DevanagariHandwrittenCharacterDataset.zip\"\n",
        "zip_file_name = hindi_handwritten_dataset_zip_url.rsplit('/', 1)[1]\n",
        "DEVANAGARI_ZIP_PATH = os.path.join(DATA_FOLDER, zip_file_name)\n",
        "DEVANAGARI_DATA_FOLDER = os.path.join(DATA_FOLDER, zip_file_name.rsplit(\".\")[0])\n",
        "\n",
        "# Ensure the data folder exists\n",
        "if not os.path.exists(DATA_FOLDER):\n",
        "    os.makedirs(DATA_FOLDER)\n",
        "\n",
        "# Download the dataset if it's not already downloaded\n",
        "if not os.path.exists(DEVANAGARI_ZIP_PATH):\n",
        "    print(\"Downloading the dataset...\")\n",
        "    req = requests.get(hindi_handwritten_dataset_zip_url, allow_redirects=True)\n",
        "    with open(DEVANAGARI_ZIP_PATH, 'wb') as output_file:\n",
        "        output_file.write(req.content)\n",
        "    print(\"Downloaded zip file.\")\n",
        "else:\n",
        "    print(\"Zip file already present.\")\n",
        "\n",
        "# Extract the dataset if it's not already extracted\n",
        "if not os.path.exists(DEVANAGARI_DATA_FOLDER):\n",
        "    with zipfile.ZipFile(DEVANAGARI_ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(DATA_FOLDER)\n",
        "    print(\"Extracted zip file.\")\n",
        "else:\n",
        "    print(\"Files already present on disk.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P00SBLo8mnc6",
        "outputId": "093d0136-9853-4341-d429-46b16a0fd247"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file already present.\n",
            "Files already present on disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data"
      ],
      "metadata": {
        "id": "4F7Giy6Bmwde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing unwanted classes\n",
        "labels_to_keep = [\n",
        "    \"digit_0\", \"digit_1\", \"digit_2\", \"digit_3\", \"digit_4\", \"digit_5\", \"digit_6\", \"digit_7\", \"digit_8\", \"digit_9\"\n",
        "]\n",
        "\n",
        "TRAIN_FOLDER_NAME = \"Train\"\n",
        "TEST_FOLDER_NAME = \"Test\"\n",
        "\n",
        "folders = glob.glob(os.path.join(DEVANAGARI_DATA_FOLDER, TRAIN_FOLDER_NAME, \"*\"))\n",
        "for f in folders:\n",
        "    if f.rsplit(\"/\")[-1] not in labels_to_keep:\n",
        "        shutil.rmtree(f)\n",
        "\n",
        "folders = glob.glob(os.path.join(DEVANAGARI_DATA_FOLDER, TEST_FOLDER_NAME, \"*\"))\n",
        "for f in folders:\n",
        "    if f.rsplit(\"/\")[-1] not in labels_to_keep:\n",
        "        shutil.rmtree(f)\n",
        "\n",
        "# Dataset and model parameters\n",
        "RANDOM_SEED = 42\n",
        "IMG_HEIGHT = 32\n",
        "IMG_WIDTH = 32\n",
        "VALIDATION_SPLIT = 0.1\n",
        "BATCH_SIZE = 32\n",
        "KERNEL_SIZE = (3, 3)\n",
        "MAX_POOLING_SIZE = (2, 2)\n",
        "DROPOUT = 0.5\n",
        "\n",
        "num_classes = len(labels_to_keep)\n",
        "classes = labels_to_keep\n",
        "classes_to_output_class_names = {\n",
        "    \"digit_0\": \"0\", \"digit_1\": \"1\", \"digit_2\": \"2\", \"digit_3\": \"3\", \"digit_4\": \"4\", \"digit_5\": \"5\", \"digit_6\": \"6\",\n",
        "    \"digit_7\": \"7\", \"digit_8\": \"8\", \"digit_9\": \"9\"\n",
        "}\n",
        "\n",
        "# Preparing datasets\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    os.path.join(DEVANAGARI_DATA_FOLDER, TRAIN_FOLDER_NAME),\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    class_names=classes,\n",
        "    color_mode=\"grayscale\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    shuffle=True,\n",
        "    seed=RANDOM_SEED,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    subset=\"training\",\n",
        ")\n",
        "\n",
        "val_dataset = image_dataset_from_directory(\n",
        "    os.path.join(DEVANAGARI_DATA_FOLDER, TRAIN_FOLDER_NAME),\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    class_names=classes,\n",
        "    color_mode=\"grayscale\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    shuffle=True,\n",
        "    seed=RANDOM_SEED,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    subset=\"validation\",\n",
        ")\n",
        "\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    os.path.join(DEVANAGARI_DATA_FOLDER, TEST_FOLDER_NAME),\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    class_names=classes,\n",
        "    color_mode=\"grayscale\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    shuffle=True,\n",
        "    seed=RANDOM_SEED,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjuGqyzYmuCq",
        "outputId": "46fc81d7-9ef6-4787-9d21-c01619d96b1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 17000 files belonging to 10 classes.\n",
            "Using 15300 files for training.\n",
            "Found 17000 files belonging to 10 classes.\n",
            "Using 1700 files for validation.\n",
            "Found 3000 files belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "gGHitVU4m0MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation and normalization\n",
        "normalization_layer = layers.Rescaling(1. / 255)\n",
        "data_augmentation_layers = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomZoom(0.05),\n",
        "        layers.RandomTranslation(0.05, 0.05),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Caching and prefetching\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "09mJDsPemzKL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build `.tf` Model"
      ],
      "metadata": {
        "id": "ZogTaY6Nm4ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        data_augmentation_layers,\n",
        "        normalization_layer,\n",
        "        layers.Conv2D(32, kernel_size=KERNEL_SIZE, activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=MAX_POOLING_SIZE),\n",
        "        layers.Conv2D(64, kernel_size=KERNEL_SIZE, activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=MAX_POOLING_SIZE),\n",
        "        layers.Dropout(DROPOUT),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compiling and training the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "epochs = 15\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IGFSdPZm3hY",
        "outputId": "510cc2aa-7fc5-4542-9348-677ca238126a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py:609: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 52ms/step - accuracy: 0.7735 - loss: 0.6808 - val_accuracy: 0.9847 - val_loss: 0.0482\n",
            "Epoch 2/15\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - accuracy: 0.9625 - loss: 0.1079 - val_accuracy: 0.9876 - val_loss: 0.0336\n",
            "Epoch 3/15\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 59ms/step - accuracy: 0.9764 - loss: 0.0760 - val_accuracy: 0.9900 - val_loss: 0.0230\n",
            "Epoch 4/15\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 47ms/step - accuracy: 0.9839 - loss: 0.0556 - val_accuracy: 0.9918 - val_loss: 0.0151\n",
            "Epoch 5/15\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.9863 - loss: 0.0448 - val_accuracy: 0.9924 - val_loss: 0.0189\n",
            "Epoch 6/15\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 50ms/step - accuracy: 0.9889 - loss: 0.0391 - val_accuracy: 0.9935 - val_loss: 0.0175\n",
            "Epoch 7/15\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 52ms/step - accuracy: 0.9876 - loss: 0.0358 - val_accuracy: 0.9947 - val_loss: 0.0111\n",
            "Epoch 8/15\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.9921 - loss: 0.0254 - val_accuracy: 0.9947 - val_loss: 0.0131\n",
            "Epoch 9/15\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 52ms/step - accuracy: 0.9911 - loss: 0.0259 - val_accuracy: 0.9953 - val_loss: 0.0184\n",
            "Epoch 10/15\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 51ms/step - accuracy: 0.9920 - loss: 0.0220 - val_accuracy: 0.9965 - val_loss: 0.0081\n",
            "Epoch 11/15\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 55ms/step - accuracy: 0.9940 - loss: 0.0195 - val_accuracy: 0.9947 - val_loss: 0.0138\n",
            "Epoch 12/15\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 50ms/step - accuracy: 0.9950 - loss: 0.0155 - val_accuracy: 0.9971 - val_loss: 0.0120\n",
            "Epoch 13/15\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 51ms/step - accuracy: 0.9960 - loss: 0.0140 - val_accuracy: 0.9947 - val_loss: 0.0182\n",
            "Epoch 14/15\n",
            "\u001b[1m139/479\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 67ms/step - accuracy: 0.9956 - loss: 0.0165"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot History"
      ],
      "metadata": {
        "id": "JvaUicGYm8FL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting accuracy and loss\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, epochs + 1)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "fig.add_subplot(1, 2, 1)\n",
        "sns.lineplot(x=epochs_range, y=acc, label='Training Accuracy')\n",
        "sns.lineplot(x=epochs_range, y=val_acc, label='Validation Accuracy')\n",
        "\n",
        "fig.add_subplot(1, 2, 2)\n",
        "sns.lineplot(x=epochs_range, y=loss, label='Training Loss')\n",
        "sns.lineplot(x=epochs_range, y=val_loss, label='Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "# Evaluating the model\n",
        "result = model.evaluate(test_dataset)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "C57SzxOEm7EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ],
      "metadata": {
        "id": "63XjBXE1m_xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model\n",
        "MODEL_FOLDER = \"./models\"\n",
        "HINDI_MNIST_FOLDER = \"hindi_mnist\"\n",
        "MODEL_SAVE_FOLDER = os.path.join(MODEL_FOLDER, HINDI_MNIST_FOLDER)\n",
        "TF_MODEL_SAVE_FOLDER = os.path.join(MODEL_FOLDER, HINDI_MNIST_FOLDER, \"tf_serving\")\n",
        "MODEL_SAVE_PATH = os.path.join(MODEL_FOLDER, HINDI_MNIST_FOLDER, \"model.h5\")\n",
        "\n",
        "model.save(MODEL_SAVE_PATH, overwrite=True, include_optimizer=True)\n",
        "model.save(TF_MODEL_SAVE_FOLDER, overwrite=True, save_format='tf')\n",
        "\n",
        "# Saving classes to a pickle file\n",
        "CLASSES_PKL_PATH = os.path.join(MODEL_SAVE_FOLDER, \"classes.pickle\")\n",
        "with open(CLASSES_PKL_PATH, 'wb') as f:\n",
        "    pkl.dump(classes, f)\n",
        "    pkl.dump(classes_to_output_class_names, f)\n",
        "\n",
        "# Loading the model and evaluating again\n",
        "model = keras.models.load_model(MODEL_SAVE_PATH)\n",
        "with open(CLASSES_PKL_PATH, 'rb') as f:\n",
        "    classes = pkl.load(f)\n",
        "    labels_to_class_names = pkl.load(f)\n",
        "\n",
        "result = model.evaluate(test_dataset)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "CYqEzRaim_C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dXZ4g0Ivkeuj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}