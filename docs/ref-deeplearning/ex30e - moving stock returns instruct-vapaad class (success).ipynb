{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Directory"
      ],
      "metadata": {
        "id": "UX36mf1dTWiZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4qUCKCE8MHa"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rJk2iUoA8Ojf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/vapaad gifs\")"
      ],
      "metadata": {
        "id": "enyB38TC8Pbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acquire Library"
      ],
      "metadata": {
        "id": "grL6SaaURIA5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0Q-giNvTJqUz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3c4178-8d65-444c-bf72-1fd547fb20ba"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.38)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.25.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.31.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.4)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2023.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.2)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf"
      ],
      "metadata": {
        "id": "ZYTY4Hj8KTnF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acquire Data"
      ],
      "metadata": {
        "id": "3F9S8A5lZfKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_stock_returns(ticker: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetches historical stock data for a given ticker and calculates the daily and weekly returns.\n",
        "\n",
        "    Parameters:\n",
        "    ticker (str): The stock ticker symbol for which to fetch data.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A DataFrame indexed by date with columns for daily and weekly returns.\n",
        "    \"\"\"\n",
        "    # Fetch historical stock data using yfinance\n",
        "    stock_data = yf.download(ticker)\n",
        "\n",
        "    # Check if data was fetched successfully\n",
        "    if stock_data.empty:\n",
        "        raise ValueError(f\"No data found for ticker {ticker}\")\n",
        "\n",
        "    # Calculate daily returns\n",
        "    stock_data['daily_ret'] = stock_data['Adj Close'].pct_change()\n",
        "\n",
        "    # Calculate weekly returns\n",
        "    stock_data['weekly_ret'] = stock_data['Adj Close'].pct_change(periods=5)\n",
        "\n",
        "    # Return the DataFrame with only the 'daily_ret' and 'weekly_ret' columns\n",
        "    return stock_data[['daily_ret', 'weekly_ret']]\n"
      ],
      "metadata": {
        "id": "4MDOFQ7tKama"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "df = fetch_stock_returns('AAPL')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "s_i_tbuZKcDS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "598a7c4b-a541-4284-eb45-2be2aa0bbb23"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10934,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1980-12-12 00:00:00\",\n        \"max\": \"2024-04-26 00:00:00\",\n        \"num_unique_values\": 10934,\n        \"samples\": [\n          \"1983-01-21 00:00:00\",\n          \"1993-05-13 00:00:00\",\n          \"1984-08-20 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"daily_ret\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.027940918658461975,\n        \"min\": -0.5186919317754808,\n        \"max\": 0.3322804896222329,\n        \"num_unique_values\": 10261,\n        \"samples\": [\n          -0.004784357656644644,\n          0.006145335335174806,\n          0.016665460949335076\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weekly_ret\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.062135712115160655,\n        \"min\": -0.5876170299976617,\n        \"max\": 0.6678590417742452,\n        \"num_unique_values\": 10644,\n        \"samples\": [\n          0.06712204053748838,\n          0.006670750130591596,\n          -0.10965828307628966\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-1dba4a82-ff90-471f-8ec7-05daa90a8be1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>daily_ret</th>\n",
              "      <th>weekly_ret</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1980-12-12</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-12-15</th>\n",
              "      <td>-0.052171</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-12-16</th>\n",
              "      <td>-0.073398</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-12-17</th>\n",
              "      <td>0.024751</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-12-18</th>\n",
              "      <td>0.028992</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1dba4a82-ff90-471f-8ec7-05daa90a8be1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1dba4a82-ff90-471f-8ec7-05daa90a8be1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1dba4a82-ff90-471f-8ec7-05daa90a8be1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-72c4b64f-d81b-4c36-ac8b-d897ad7a5d27\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-72c4b64f-d81b-4c36-ac8b-d897ad7a5d27')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-72c4b64f-d81b-4c36-ac8b-d897ad7a5d27 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "            daily_ret  weekly_ret\n",
              "Date                             \n",
              "1980-12-12        NaN         NaN\n",
              "1980-12-15  -0.052171         NaN\n",
              "1980-12-16  -0.073398         NaN\n",
              "1980-12-17   0.024751         NaN\n",
              "1980-12-18   0.028992         NaN"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "D6C2BtHHKeSC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_stock_motion_figures(df: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Creates binary scatter plot images of daily and weekly returns from a pandas DataFrame\n",
        "    and compiles these into a GIF.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame containing 'daily_ret' and 'weekly_ret' columns.\n",
        "\n",
        "    Returns:\n",
        "    np.ndarray: An array of binary images with shape (None, 64, 64).\n",
        "    \"\"\"\n",
        "    # Remove rows with NaN values\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Prepare an array to hold images\n",
        "    images = []\n",
        "\n",
        "    # Loop over the DataFrame to create scatter plots\n",
        "    scaling_factor = 0.6\n",
        "    for i in tqdm(range(len(df.head(2000)))):\n",
        "        fig, ax = plt.subplots()\n",
        "        # Scatter plot for the current row\n",
        "        ax.scatter(df.iloc[i]['daily_ret'], df.iloc[i]['weekly_ret'], color='black', s=500, marker='o')\n",
        "        ax.set_xlim(df['daily_ret'].min()*scaling_factor, df['daily_ret'].max()*scaling_factor)\n",
        "        ax.set_ylim(df['weekly_ret'].min()*scaling_factor, df['weekly_ret'].max()*scaling_factor)\n",
        "        ax.axis('off')  # Turn off the axis\n",
        "\n",
        "        # Convert plot to image\n",
        "        canvas = FigureCanvas(fig)\n",
        "        ax.figure.canvas.draw()\n",
        "        image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
        "        image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "\n",
        "        # Convert to binary image\n",
        "        image = Image.fromarray(image)\n",
        "        image = image.convert('L').point(lambda x: 0 if x < 128 else 255, '1')\n",
        "        image = image.resize((64, 64), Image.LANCZOS)\n",
        "        image_array = np.array(image)\n",
        "        images.append(image_array)\n",
        "\n",
        "        # Clear the plot figure to free memory\n",
        "        plt.close(fig)\n",
        "\n",
        "    # Save the images as a GIF\n",
        "    image_list = [Image.fromarray(img.astype('uint8') * 255) for img in images]\n",
        "    image_list[0].save('stock_movement.gif', save_all=True, append_images=image_list[1:], duration=200, loop=0)\n",
        "\n",
        "    return np.array(images)"
      ],
      "metadata": {
        "id": "s5TXbOrML3t1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch Data"
      ],
      "metadata": {
        "id": "5l1A2jAC2mYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "df = fetch_stock_returns('AAPL')\n",
        "motion_images = create_stock_motion_figures(df)\n",
        "print(motion_images.shape)"
      ],
      "metadata": {
        "id": "Zh55l9vyL4ZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f81fe3-4311-4ef4-987a-244b04691f18"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "100%|██████████| 2000/2000 [01:07<00:00, 29.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2000, 64, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_sequences(data: np.ndarray) -> tuple:\n",
        "    \"\"\"\n",
        "    Generates sequential batches of images from the provided array.\n",
        "\n",
        "    Parameters:\n",
        "    data (np.ndarray): Input array of shape (None, 64, 64), where None represents any number of 64x64 images.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing two numpy arrays, both of shape (None, 19, 64, 64, 1).\n",
        "           The first array 'x' contains sequences of 19 consecutive frames, and the second array 'y' contains the subsequent 19 frames.\n",
        "    \"\"\"\n",
        "    num_images = data.shape[0]\n",
        "    sequence_length = 19\n",
        "\n",
        "    # Initialize lists to hold the sequences\n",
        "    x, y = [], []\n",
        "\n",
        "    # Generate sequences\n",
        "    for i in range(num_images - 2 * sequence_length):\n",
        "        x_sequence = data[i:i + sequence_length]  # Slice 19 samples for x\n",
        "        y_sequence = data[i + sequence_length:i + 2 * sequence_length]  # Slice next 19 samples for y\n",
        "\n",
        "        # Reshape sequences and add a channel dimension\n",
        "        x_sequence = x_sequence.reshape((sequence_length, 64, 64, 1))\n",
        "        y_sequence = y_sequence.reshape((sequence_length, 64, 64, 1))\n",
        "\n",
        "        # Append to lists\n",
        "        x.append(x_sequence)\n",
        "        y.append(y_sequence)\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "cPZK_-eFL5oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "# Assuming 'input_array' is your initial (None, 64, 64) array loaded or created earlier.\n",
        "x, y = create_sequences(motion_images)\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "afLYstToQzml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Test Split"
      ],
      "metadata": {
        "id": "pcOw1ze92nxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "R1Q44kqv2VHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build `VAPAAD` Model"
      ],
      "metadata": {
        "id": "pmuOL5DMTfDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "class SelfAttention(layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom self-attention layer that computes attention scores to enhance model performance by focusing on relevant parts of the input data.\n",
        "\n",
        "    This layer creates query, key, and value representations of the input, then calculates attention scores to determine how much focus to put on each part of the input data. The output is a combination of the input and the attention mechanism's weighted focus, which allows the model to pay more attention to certain parts of the data.\n",
        "\n",
        "    Attributes:\n",
        "        query_dense (keras.layers.Dense): A dense layer for transforming the input into a query tensor.\n",
        "        key_dense (keras.layers.Dense): A dense layer for transforming the input into a key tensor.\n",
        "        value_dense (keras.layers.Dense): A dense layer for transforming the input into a value tensor.\n",
        "        combine_heads (keras.layers.Dense): A dense layer for combining the attention heads' outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SelfAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape: Tuple[int, ...]):\n",
        "        \"\"\"\n",
        "        Initializes the internal dense layers based on the last dimension of the input shape, setting up the query, key, value, and combine heads layers.\n",
        "\n",
        "        Args:\n",
        "            input_shape (Tuple[int, ...]): The shape of the input tensor to the layer.\n",
        "        \"\"\"\n",
        "        self.query_dense = layers.Dense(units=input_shape[-1])\n",
        "        self.key_dense = layers.Dense(units=input_shape[-1])\n",
        "        self.value_dense = layers.Dense(units=input_shape[-1])\n",
        "        self.combine_heads = layers.Dense(units=input_shape[-1])\n",
        "\n",
        "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Performs the self-attention mechanism on the input tensor and returns the combined output with a residual connection.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): The input tensor to the self-attention layer.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The output tensor after applying self-attention and combining with the input tensor through a residual connection.\n",
        "        \"\"\"\n",
        "        # Generate query, key, value tensors\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        scores = tf.matmul(query, key, transpose_b=True)\n",
        "        distribution = tf.nn.softmax(scores)\n",
        "        attention_output = tf.matmul(distribution, value)\n",
        "\n",
        "        # Combine heads and add residual connection\n",
        "        combined_output = self.combine_heads(attention_output) + inputs\n",
        "        return combined_output\n",
        "\n",
        "\n",
        "class VAPAAD:\n",
        "    \"\"\"\n",
        "    The VAPAAD model, short for Vision Augmentation Prediction Autoencoder with Attention Design,\n",
        "    is a sophisticated neural network architecture tailored for video processing tasks. This model\n",
        "    leverages a dual-encoder structure to process sequences of video frames for tasks such as\n",
        "    video frame prediction and unsupervised learning. The unique aspect of this architecture is its\n",
        "    stop gradient design, which effectively separates the learning phases of the two encoders, allowing\n",
        "    one encoder to stabilize while the other continues to adapt during training.\n",
        "\n",
        "    This architecture integrates data augmentation directly into the video processing pipeline,\n",
        "    enhancing the model's ability to generalize across varied video data. It employs self-attention\n",
        "    mechanisms to capture long-range dependencies within the video sequences, thereby enhancing its\n",
        "    predictive capabilities.\n",
        "\n",
        "    Attributes:\n",
        "        input_shape (Tuple[int, int, int]): The shape of the input frames expected by the model.\n",
        "        gen_main (keras.Model): The main generator model that processes the input frames.\n",
        "        gen_aux (keras.Model): An auxiliary generator used to predict future frames.\n",
        "        instructor (keras.Model): The instructor model that evaluates the generated frames.\n",
        "        cross_entropy (tf.keras.losses.Loss): Loss function used for training the model.\n",
        "        generator_optimizer (tf.keras.optimizers.Optimizer): Optimizer for the generator models.\n",
        "        instructor_optimizer (tf.keras.optimizers.Optimizer): Optimizer for the instructor model.\n",
        "\n",
        "    The VAPAAD model is developed by Yiqiao Yin, who can be reached at eagle0504@gmail.com for further\n",
        "    inquiries or support related to this implementation.\n",
        "\n",
        "    Example usage:\n",
        "        # Initializing a new VAPAAD model\n",
        "        vapaad_model = VAPAAD(input_shape=(19, 64, 64, 1))\n",
        "\n",
        "        # Assuming x_train and y_train are already defined and loaded\n",
        "        vapaad_model.train(x_train, y_train, batch_size=32)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: Tuple[int, int, int]):\n",
        "        self.input_shape = input_shape\n",
        "        # Initialize generator and instructor models\n",
        "        self.gen_main = self.build_generator()\n",
        "        self.gen_aux = self.build_generator()\n",
        "        self.instructor = self.build_instructor()\n",
        "        # Define loss functions and optimizers\n",
        "        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        self.learning_rate = tf.Variable(1e-4, trainable=False)\n",
        "        self.generator_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        self.instructor_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "    def build_generator(self) -> keras.Model:\n",
        "        \"\"\"\n",
        "        Constructs the generator model for video processing with data augmentation and self-attention.\n",
        "\n",
        "        This method is responsible for creating a generator model that performs augmentations on input\n",
        "        frames and then processes them through ConvLSTM2D layers with self-attention, finally applying a\n",
        "        convolution across the time dimension to generate output frames.\n",
        "\n",
        "        The model is part of a generative approach and could be used in tasks such as video frame prediction,\n",
        "        unsupervised learning, or as a part of a Generative Adversarial Network (GAN).\n",
        "\n",
        "        Returns:\n",
        "            A Keras model that takes a sequence of frames as input, augments them via random zooming, rotations,\n",
        "            and translations, and then outputs processed frames with the same sequence length as the input.\n",
        "\n",
        "        Note: 'input_shape' should be an attribute of the class instance, and 'SelfAttention' is expected\n",
        "        to be either a predefined layer in Keras or a custom implementation provided in the code.\n",
        "\n",
        "        Example usage:\n",
        "            generator = build_generator()\n",
        "        \"\"\"\n",
        "        # Data augmentation layers intended to increase robustness and generalization\n",
        "        data_augmentation = keras.Sequential(\n",
        "            [\n",
        "                # layers.RandomZoom(height_factor=0.00002, width_factor=0.00002),\n",
        "                # layers.RandomRotation(factor=0.02),\n",
        "                layers.RandomTranslation(height_factor=0.02, width_factor=0.02),\n",
        "            ],\n",
        "            name=\"data_augmentation\",\n",
        "        )\n",
        "\n",
        "        # Input layer defining the shape of the input frames\n",
        "        inp = layers.Input(shape=self.input_shape)\n",
        "        # Apply time distributed data augmentation which applies the augmentation to each frame independently\n",
        "        x = layers.TimeDistributed(data_augmentation)(inp)\n",
        "        # Convolutional LSTM layer with relu activation to capture temporal features\n",
        "        x = layers.ConvLSTM2D(\n",
        "            filters=64,\n",
        "            kernel_size=(5, 5),\n",
        "            padding=\"same\",\n",
        "            return_sequences=True,\n",
        "            activation=\"relu\",\n",
        "        )(x)\n",
        "        # Batch normalization to help maintain the stability of the network\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        # Self-attention layer for capturing long-range dependencies within the sequences\n",
        "        x = SelfAttention()(x)\n",
        "        # Conv3D layer to process the features obtained from previous layers and produce a sequence of frames\n",
        "        x = layers.Conv3D(\n",
        "            filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
        "        )(x)\n",
        "\n",
        "        # Construct the model with the specified input and output tensors\n",
        "        return keras.models.Model(inputs=inp, outputs=x)\n",
        "\n",
        "    def build_instructor(self) -> keras.Model:\n",
        "        \"\"\"\n",
        "        Constructs the instructor model with convolutional LSTM and fully connected layers.\n",
        "\n",
        "        This method specifically builds a video processing instructor model that uses ConvLSTM2D layers,\n",
        "        followed by self-attention, global average pooling, and dense layers to process the input frames\n",
        "        and predict a one-dimensional output.\n",
        "\n",
        "        The architecture is designed for sequential data processing ideal for video or time-series data.\n",
        "\n",
        "        Returns:\n",
        "            A compiled Keras model that takes a sequence of frames as input and outputs a\n",
        "            one-dimensional tensor after processing through ConvLSTM2D, self-attention,\n",
        "            and dense layers. The output can be interpreted as the probability of a certain\n",
        "            class or a value depending on the final activation function used (sigmoid in this case).\n",
        "\n",
        "        Note: 'input_shape' should be an attribute of the class instance, and 'SelfAttention' is\n",
        "        assumed to be a pre-defined layer or a custom layer implemented elsewhere in the code.\n",
        "\n",
        "        Example usage:\n",
        "            model = build_instructor()\n",
        "        \"\"\"\n",
        "        # Input layer defining the shape of the input frames\n",
        "        inp = layers.Input(shape=self.input_shape)\n",
        "        # Convolutional LSTM layer with relu activation\n",
        "        x = layers.ConvLSTM2D(\n",
        "            filters=64,\n",
        "            kernel_size=(3, 3),\n",
        "            padding=\"same\",\n",
        "            return_sequences=True,\n",
        "            activation=\"relu\",\n",
        "        )(inp)\n",
        "        # Batch Normalization layer\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        # Self-attention layer for sequence learning\n",
        "        x = SelfAttention()(x)\n",
        "        # Global Average Pooling across the frames to get a feature vector\n",
        "        x = layers.GlobalAveragePooling3D()(x)\n",
        "        # Fully connected layers with relu activation\n",
        "        x = layers.Dense(1024, activation=\"relu\")(x)\n",
        "        x = layers.Dense(512, activation=\"relu\")(x)\n",
        "        # Output layer with sigmoid activation for binary classification or regression tasks\n",
        "        output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "        # Construct the model with specified layers\n",
        "        return keras.models.Model(inputs=inp, outputs=output)\n",
        "\n",
        "    def update_learning_rate(self, current_loss: float, previous_loss: float, decay_rate: float = 0.9) -> None:\n",
        "        \"\"\"\n",
        "        Adjusts the learning rate based on the comparison between current and previous loss.\n",
        "\n",
        "        The learning rate is reduced by 10% if the current loss is greater than the previous loss,\n",
        "        ensuring that the learning rate does not fall below a minimum threshold of 1e-6.\n",
        "        \"\"\"\n",
        "        # Check if the current loss has increased compared to the previous loss\n",
        "        if current_loss > previous_loss:\n",
        "            # Calculate the new learning rate by reducing the current learning rate by 10%\n",
        "            new_lr = self.learning_rate * decay_rate\n",
        "            # Update the learning rate with the higher value between the new learning rate and the minimum threshold\n",
        "            self.learning_rate.assign(max(new_lr, 1e-6))\n",
        "\n",
        "    def train_step(\n",
        "        self, images: tf.Tensor, future_images: tf.Tensor\n",
        "    ) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "        \"\"\"\n",
        "        Perform a single training step by updating the generator and instructor models.\n",
        "\n",
        "        This method applies gradient descent to both the generator and the instructor models\n",
        "        based on the loss computed from the real and generated images.\n",
        "\n",
        "        Args:\n",
        "            images (tf.Tensor): A tensor of input images for the current time step provided\n",
        "                                to the generator model 'gen_main'.\n",
        "            future_images (tf.Tensor): A tensor of target images for the future time step provided\n",
        "                                    to the generator model 'gen_aux'.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[tf.Tensor, tf.Tensor]: A tuple containing the loss values for the generator model\n",
        "                                        ('gen_loss') and the instructor model ('inst_loss').\n",
        "\n",
        "        Note: 'gen_optimizer' and 'inst_optimizer' should be attributes of the class instance.\n",
        "\n",
        "        The function uses TensorFlow operations and assumes that 'gen_main', 'gen_aux', 'instructor',\n",
        "        'generator_optimizer', 'instructor_optimizer', 'generator_loss', and 'instructor_loss' are\n",
        "        defined as attributes of the class in which this method is implemented.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as inst_tape:\n",
        "            # Generate outputs for both current and future inputs\n",
        "            output_main = self.gen_main(images, training=True)\n",
        "            output_aux = self.gen_aux(future_images, training=True)\n",
        "            real_output = self.instructor(output_aux, training=True)\n",
        "            fake_output = self.instructor(output_main, training=True)\n",
        "\n",
        "            # Calculate losses for both models\n",
        "            gen_loss = self.generator_loss(fake_output)\n",
        "            inst_loss = self.instructor_loss(real_output, fake_output)\n",
        "\n",
        "        # Apply gradients to update model weights\n",
        "        gradients_of_gen = gen_tape.gradient(\n",
        "            gen_loss, self.gen_main.trainable_variables\n",
        "        )\n",
        "        gradients_of_inst = inst_tape.gradient(\n",
        "            inst_loss, self.instructor.trainable_variables\n",
        "        )\n",
        "        self.generator_optimizer.apply_gradients(\n",
        "            zip(gradients_of_gen, self.gen_main.trainable_variables)\n",
        "        )\n",
        "        self.instructor_optimizer.apply_gradients(\n",
        "            zip(gradients_of_inst, self.instructor.trainable_variables)\n",
        "        )\n",
        "\n",
        "        return gen_loss, inst_loss\n",
        "\n",
        "    def generator_loss(self, fake_output):\n",
        "        \"\"\"\n",
        "        Calculates the loss for the generator model based on its output for generated (fake) images.\n",
        "\n",
        "        The loss encourages the generator to produce images that the instructor model classifies as real.\n",
        "        This is achieved by comparing the generator's output for fake images against a target tensor of ones,\n",
        "        indicating that the ideal output of the generator would be classified as real by the instructor model.\n",
        "\n",
        "        Args:\n",
        "        fake_output (tf.Tensor): The generator model's output logits for generated (fake) images.\n",
        "\n",
        "        Returns:\n",
        "        tf.Tensor: The loss for the generator model, encouraging it to generate more realistic images.\n",
        "        \"\"\"\n",
        "        return self.cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "    def instructor_loss(self, real_output, fake_output):\n",
        "        \"\"\"\n",
        "        Calculates the loss for the instructor model based on its output for real and generated (fake) images.\n",
        "\n",
        "        The loss is computed as the sum of the cross-entropy losses for the real and fake outputs. For real images,\n",
        "        the target is a tensor of ones, and for fake images, the target is a tensor of zeros.\n",
        "\n",
        "        Args:\n",
        "        real_output (tf.Tensor): The instructor model's output logits for real images.\n",
        "        fake_output (tf.Tensor): The instructor model's output logits for generated (fake) images.\n",
        "\n",
        "        Returns:\n",
        "        tf.Tensor: The total loss for the instructor model, combining the real and fake loss components.\n",
        "        \"\"\"\n",
        "        # Define real_loss and fake_loss\n",
        "        real_loss = self.cross_entropy(tf.zeros_like(real_output), real_output)\n",
        "        fake_loss = self.cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "        return real_loss + fake_loss\n",
        "\n",
        "    def train(self, x_train, y_train, batch_size=64):\n",
        "        \"\"\"\n",
        "        Trains the model for a specified batch size.\n",
        "\n",
        "        This function iterates over the entire dataset for a epoch,\n",
        "        randomly selecting batches of data to perform training steps. The selection is random\n",
        "        and without replacement within each epoch, ensuring diverse exposure of data.\n",
        "\n",
        "        Args:\n",
        "        x_train (np.ndarray): The input training data.\n",
        "        y_train (np.ndarray): The target training data.\n",
        "        batch_size (int, optional): The number of samples per batch of computation. Defaults to 64.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        previous_loss = float('inf')\n",
        "        n_samples = x_train.shape[0]\n",
        "        start = time.time()\n",
        "        indices = np.arange(n_samples)\n",
        "        np.random.shuffle(indices)\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            if i + batch_size > n_samples:\n",
        "                continue  # Avoid index error on the last batch if it's smaller than the batch size\n",
        "            selected_indices = indices[i : i + batch_size]\n",
        "            x_batch = x_train[selected_indices]\n",
        "            y_batch = y_train[selected_indices]\n",
        "            curr_gen_loss, curr_inst_loss = self.train_step(x_batch, y_batch)\n",
        "            # if curr_gen_loss < 0.2:  # Early stopping condition\n",
        "            #     print(\n",
        "            #         f\"> running: current sample {i + 1}, gen_loss={curr_gen_loss}, inst_loss={curr_inst_loss}, time={time.time() - start} sec\"\n",
        "            #     )\n",
        "            #     return\n",
        "\n",
        "            # Update learning rate based on the loss\n",
        "            self.update_learning_rate(curr_gen_loss, previous_loss)\n",
        "            previous_loss = curr_gen_loss\n",
        "\n",
        "            print(\n",
        "                f\"> running: current sample {i + 1}, gen_loss={curr_gen_loss}, inst_loss={curr_inst_loss}, time={time.time() - start} sec\"\n",
        "            )\n",
        "\n",
        "    def __read_me__(self):\n",
        "        \"\"\"\n",
        "        This function prints a multi-line formatted instruction manual for running a VAPAAD model.\n",
        "\n",
        "        The instructions include how to inspect the data shapes of training and validation datasets,\n",
        "        initializing the VAPAAD model, selecting a random subset of the training data for training,\n",
        "        and finally, running the model with GPU support if available.\n",
        "\n",
        "        There are no parameters for this function and it doesn't return anything.\n",
        "        It simply prints the instructional text to the console when called.\n",
        "        \"\"\"\n",
        "        now = datetime.now()\n",
        "        current_year = now.year\n",
        "        print(\n",
        "            f\"\"\"\n",
        "            ## Instructions\n",
        "\n",
        "            Assume you have data as the follows:\n",
        "\n",
        "            ```py\n",
        "            # Inspect the dataset.\n",
        "            print(\"Training Dataset Shapes: \" + str(x_train.shape) + \", \" + str(y_train.shape))\n",
        "            print(\"Validation Dataset Shapes: \" + str(x_val.shape) + \", \" + str(y_val.shape))\n",
        "\n",
        "            # output\n",
        "            # Training Dataset Shapes: (900, 19, 64, 64, 1), (900, 19, 64, 64, 1)\n",
        "            # Validation Dataset Shapes: (100, 19, 64, 64, 1), (100, 19, 64, 64, 1)\n",
        "            ```\n",
        "\n",
        "            To run the model, execute the following:\n",
        "            ```py\n",
        "            # Initializing a new VAPAAD model\n",
        "            vapaad_model = VAPAAD(input_shape=(19, 64, 64, 1))\n",
        "\n",
        "            # Assuming x_train and y_train are already defined and loaded\n",
        "            num_samples = 64\n",
        "            indices = np.random.choice(x_train.shape[0], num_samples, replace=True)\n",
        "            print(indices[0:6])\n",
        "            x_train_sub = x_train[indices]\n",
        "            y_train_sub = y_train[indices]\n",
        "            print(x_train_sub.shape, y_train_sub.shape)\n",
        "\n",
        "            # Example usage:\n",
        "            BATCH_SIZE = 3\n",
        "            if tf.test.gpu_device_name() != '':\n",
        "                with tf.device('/device:GPU:0'):\n",
        "                    vapaad_model.train(x_train_sub, y_train_sub, batch_size=BATCH_SIZE)\n",
        "            else:\n",
        "                vapaad_model.train(x_train_sub, y_train_sub, batch_size=BATCH_SIZE)\n",
        "            ```\n",
        "\n",
        "            Copyright © 2010-{current_year} Present Yiqiao Yin\n",
        "            \"\"\"\n",
        "        )"
      ],
      "metadata": {
        "id": "B6scdX2cRpuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert to `16bit` data"
      ],
      "metadata": {
        "id": "Duj-MHwiXogB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data to a smaller data type to reduce memory usage\n",
        "x_train = x_train.astype(np.float16)\n",
        "y_train = y_train.astype(np.float16)"
      ],
      "metadata": {
        "id": "6KtEpWygSSX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of x', x_train.shape)\n",
        "print('Shape of y', y_train.shape)"
      ],
      "metadata": {
        "id": "-GGwAGt6Res1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check gpu\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "DoL9b86_SUEB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "15e988e6-766e-4872-8ef6-aa6139709233"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming x_train and y_train are already defined and loaded\n",
        "num_samples = 64\n",
        "indices = np.random.choice(x_train.shape[0], num_samples, replace=False)\n",
        "print(indices[0:6])\n",
        "\n",
        "x_train_sub = x_train[indices]\n",
        "y_train_sub = y_train[indices]\n",
        "print(x_train_sub.shape, y_train_sub.shape)"
      ],
      "metadata": {
        "id": "M_Ry6pDaSU6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f9187e-0bb6-4991-fe95-dedfc0b2ad53"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1224  120 1289 1158 1419 1054]\n",
            "(64, 19, 64, 64, 1) (64, 19, 64, 64, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define model: `VAPAAD`"
      ],
      "metadata": {
        "id": "HBC-SyfwSajv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing a new VAPAAD model\n",
        "vapaad_model = VAPAAD(input_shape=(19, 64, 64, 1))"
      ],
      "metadata": {
        "id": "UBrG2Q5BRqQ-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Readme\n",
        "vapaad_model.__read_me__()"
      ],
      "metadata": {
        "id": "kmAKR6ZGZRhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f181cbff-bc11-43b4-ac23-a1006f4b6d9d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "            ## Instructions\n",
            "\n",
            "            Assume you have data as the follows:\n",
            "\n",
            "            ```py\n",
            "            # Inspect the dataset.\n",
            "            print(\"Training Dataset Shapes: \" + str(x_train.shape) + \", \" + str(y_train.shape))\n",
            "            print(\"Validation Dataset Shapes: \" + str(x_val.shape) + \", \" + str(y_val.shape))\n",
            "\n",
            "            # output\n",
            "            # Training Dataset Shapes: (900, 19, 64, 64, 1), (900, 19, 64, 64, 1)\n",
            "            # Validation Dataset Shapes: (100, 19, 64, 64, 1), (100, 19, 64, 64, 1)\n",
            "            ```\n",
            "\n",
            "            To run the model, execute the following:\n",
            "            ```py\n",
            "            # Initializing a new VAPAAD model\n",
            "            vapaad_model = VAPAAD(input_shape=(19, 64, 64, 1))\n",
            "\n",
            "            # Assuming x_train and y_train are already defined and loaded\n",
            "            num_samples = 64\n",
            "            indices = np.random.choice(x_train.shape[0], num_samples, replace=True)\n",
            "            print(indices[0:6])\n",
            "            x_train_sub = x_train[indices]\n",
            "            y_train_sub = y_train[indices]\n",
            "            print(x_train_sub.shape, y_train_sub.shape)\n",
            "\n",
            "            # Example usage:\n",
            "            BATCH_SIZE = 3\n",
            "            if tf.test.gpu_device_name() != '':\n",
            "                with tf.device('/device:GPU:0'):\n",
            "                    vapaad_model.train(x_train_sub, y_train_sub, batch_size=BATCH_SIZE)\n",
            "            else:\n",
            "                vapaad_model.train(x_train_sub, y_train_sub, batch_size=BATCH_SIZE)\n",
            "            ```\n",
            "\n",
            "            Copyright © 2010-2024 Present Yiqiao Yin\n",
            "            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "FXmWEterWDQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Assuming x_train and y_train are already defined and loaded\n",
        "num_samples = 64\n",
        "indices = np.random.choice(x_train.shape[0], num_samples, replace=True)\n",
        "print(indices[0:6])\n",
        "x_train_sub = x_train[indices]\n",
        "y_train_sub = y_train[indices]\n",
        "print(x_train_sub.shape, y_train_sub.shape)\n",
        "\n",
        "# Example usage:\n",
        "BATCH_SIZE = 3\n",
        "if tf.test.gpu_device_name() != '':\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        vapaad_model.train(x_train_sub, y_train_sub, batch_size=BATCH_SIZE)\n",
        "else:\n",
        "    vapaad_model.train(x_train_sub, y_train_sub, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "xvMgsvmARmWx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c5ec3c-52ef-4dc8-ccde-dac1df108ae6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 369  950 1115  442 1180   87]\n",
            "(64, 19, 64, 64, 1) (64, 19, 64, 64, 1)\n",
            "> running: current sample 1, gen_loss=0.07454424351453781, inst_loss=0.3413679301738739, time=1.598245620727539 sec\n",
            "> running: current sample 4, gen_loss=0.056710317730903625, inst_loss=0.3012571632862091, time=3.0827035903930664 sec\n",
            "> running: current sample 7, gen_loss=0.04287955164909363, inst_loss=0.26556217670440674, time=4.92992901802063 sec\n",
            "> running: current sample 10, gen_loss=0.03210516646504402, inst_loss=0.23327913880348206, time=6.476662635803223 sec\n",
            "> running: current sample 13, gen_loss=0.024524062871932983, inst_loss=0.20509658753871918, time=8.004306554794312 sec\n",
            "> running: current sample 16, gen_loss=0.01885112002491951, inst_loss=0.17940974235534668, time=9.548813104629517 sec\n",
            "> running: current sample 19, gen_loss=0.014498722739517689, inst_loss=0.15646301209926605, time=11.067357063293457 sec\n",
            "> running: current sample 22, gen_loss=0.01125557441264391, inst_loss=0.13583503663539886, time=12.586242914199829 sec\n",
            "> running: current sample 25, gen_loss=0.008986481465399265, inst_loss=0.11735273152589798, time=14.068797826766968 sec\n",
            "> running: current sample 28, gen_loss=0.007236534263938665, inst_loss=0.10104771703481674, time=15.59657621383667 sec\n",
            "> running: current sample 31, gen_loss=0.005868325475603342, inst_loss=0.08649656921625137, time=17.109636783599854 sec\n",
            "> running: current sample 34, gen_loss=0.00479468097910285, inst_loss=0.07383611053228378, time=18.612475633621216 sec\n",
            "> running: current sample 37, gen_loss=0.003969419747591019, inst_loss=0.06286416947841644, time=20.113759517669678 sec\n",
            "> running: current sample 40, gen_loss=0.00333065795712173, inst_loss=0.053254228085279465, time=21.642026662826538 sec\n",
            "> running: current sample 43, gen_loss=0.0028262054547667503, inst_loss=0.04496682435274124, time=23.20128035545349 sec\n",
            "> running: current sample 46, gen_loss=0.002421746263280511, inst_loss=0.03790534287691116, time=24.756949424743652 sec\n",
            "> running: current sample 49, gen_loss=0.002082184189930558, inst_loss=0.031845249235630035, time=26.278563261032104 sec\n",
            "> running: current sample 52, gen_loss=0.0018039523856714368, inst_loss=0.026681382209062576, time=27.761844396591187 sec\n",
            "> running: current sample 55, gen_loss=0.001587206614203751, inst_loss=0.02221365086734295, time=29.25363039970398 sec\n",
            "> running: current sample 58, gen_loss=0.0014049591263756156, inst_loss=0.018499353900551796, time=30.74240732192993 sec\n",
            "> running: current sample 61, gen_loss=0.0012600828194990754, inst_loss=0.015402429737150669, time=32.23247194290161 sec\n",
            "CPU times: user 32.2 s, sys: 359 ms, total: 32.6 s\n",
            "Wall time: 32.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract generator"
      ],
      "metadata": {
        "id": "raN7glbCT6RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a method to get the trained generator\n",
        "trained_generator = vapaad_model.gen_main"
      ],
      "metadata": {
        "id": "B9gvj3IUSgLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction Using Generator"
      ],
      "metadata": {
        "id": "7Qk6jXY_YH_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val, y_val = x_test, y_test"
      ],
      "metadata": {
        "id": "5jfc23hVSbRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# prediction on validation set\n",
        "if tf.test.gpu_device_name() != '':\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        y_val_pred = trained_generator.predict(x_val)\n",
        "else:\n",
        "    y_val_pred = trained_generator.predict(x_val)\n",
        "\n",
        "print(\"Shape of true y_val:\", y_val.shape)\n",
        "print(\"Shape of predicted y_val:\", y_val_pred.shape)"
      ],
      "metadata": {
        "id": "MNikg6yYUAO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "xd1dv8mrYLCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def rescale_and_discretize(image):\n",
        "    \"\"\"\n",
        "    Rescales an image to the 0-1 range and discretizes the values into levels {0.1, 0.2, ..., 0.9}.\n",
        "\n",
        "    Args:\n",
        "    image (np.ndarray): The input image to be rescaled and discretized.\n",
        "\n",
        "    Returns:\n",
        "    np.ndarray: The rescaled and discretized image.\n",
        "    \"\"\"\n",
        "    # Rescale image to 0-1 range\n",
        "    min_val = np.min(image)\n",
        "    max_val = np.max(image)\n",
        "    normalized_image = (image - min_val) / (max_val - min_val) if max_val > min_val else image\n",
        "\n",
        "    # Discretize to nearest levels in {0.1, 0.2, ..., 0.9}\n",
        "    Q = 32  # Number of levels\n",
        "    discretized_image = np.round(normalized_image * Q) / (Q+1)  # Multiplies by 9, rounds, then divides by 10\n",
        "    discretized_image = np.clip(discretized_image, 0.1, 0.9)  # Ensures values are within the specified levels\n",
        "\n",
        "    return discretized_image"
      ],
      "metadata": {
        "id": "lhOXV6U9T24x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set up the figure with specified dimensions\n",
        "plt.figure(figsize=(20, 6))\n",
        "\n",
        "# Randomly select 5 samples\n",
        "num_samples = 2\n",
        "indices = np.random.choice(y_val.shape[0], num_samples, replace=False)\n",
        "\n",
        "# Total number of plots per sample (19 predictions + 19 actuals)\n",
        "num_plots_per_sample = 19 * 2  # 19 predicted and 19 actual\n",
        "\n",
        "# Iterate over each of the randomly selected samples\n",
        "for idx, sample_index in enumerate(indices):\n",
        "    # First row for y_val (true values)\n",
        "    for i in range(19):\n",
        "        ax = plt.subplot(num_samples * 2, 19, 2 * idx * 19 + i + 1)  # Calculate position index\n",
        "        ax.imshow(y_val[sample_index, i, :, :, 0], cmap='gray')\n",
        "        ax.axis('off')\n",
        "        if i == 0:\n",
        "            ax.set_title(f'Sample {sample_index+1} - True Frames')\n",
        "        ax.text(0.5, -0.1, f't={i+1}', ha='center', va='center', transform=ax.transAxes, fontsize=8)  # Adding time label\n",
        "\n",
        "    # Second row for y_val_pred (predicted values)\n",
        "    for i in range(19):\n",
        "        ax = plt.subplot(num_samples * 2, 19, (2 * idx + 1) * 19 + i + 1)  # Calculate position index\n",
        "        image = y_val_pred[sample_index, i, :, :, 0]\n",
        "        result_image = rescale_and_discretize(image)\n",
        "        ax.imshow(result_image, cmap='gray')\n",
        "        ax.axis('off')\n",
        "        if i == 0:\n",
        "            ax.set_title(f'Sample {sample_index+1} - Pred Frames')\n",
        "        ax.text(0.5, -0.1, f't={i+19+1}', ha='center', va='center', transform=ax.transAxes, fontsize=8)  # Adjusted text position\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xvM2YAjvT8su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "53psKctfZh-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X-axis: Min return\", df.daily_ret.min(), \"Max return\", df.daily_ret.max())\n",
        "print(\"Y-axis: Min return\", df.weekly_ret.min(), \"Max return\", df.weekly_ret.max())"
      ],
      "metadata": {
        "id": "Li4-Za58Zjrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "print(\"Today's shape:\", y_val_pred[-1][i].shape)\n",
        "print(\"Tomorrow's shape:\", y_val_pred[-1][i+1].shape)"
      ],
      "metadata": {
        "id": "rtaH0KA8SfXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total length:\", len(y_val_pred[-1]))"
      ],
      "metadata": {
        "id": "PDV5qTGpctJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "delta=1\n",
        "plt.imshow(y_val_pred[-1][i+delta] > np.mean(y_val_pred[-1][i+delta]), cmap='gray')"
      ],
      "metadata": {
        "id": "TQYT4yh3dkV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "delta=8\n",
        "L=len(y_val_pred[-1])\n",
        "\n",
        "plt.figure(figsize=(24, 6))\n",
        "for delta in range(L):\n",
        "    plt.subplot(1, L, delta+1)\n",
        "    plt.imshow(y_val_pred[-1][i+delta] > np.mean(y_val_pred[-1][i+delta]), cmap='gray')\n",
        "    plt.title(f't+{delta+1}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jst2TvAhY_M8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "f83c10bd-64fa-43f5-b96c-418156357892"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2400x600 with 19 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAB1gAAAB8CAYAAADEk2KXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7dUlEQVR4nO3dd3gU9f7+/3tTydJCgIQiSjAQJFL10AxEgYPAEVCMAiKIimKlKKIo0hSBIwpYQEBFkaoUQQQUEKWJJyKo8EFDkRpCCEmoIXV+f/DLfokBDMnszmb3+biuXLqzu+95TW5mN7OvnffYDMMwBAAAAAAAAAAAAAD4Rz5WFwAAAAAAAAAAAAAAJQUNVgAAAAAAAAAAAAAoJBqsAAAAAAAAAAAAAFBINFgBAAAAAAAAAAAAoJBosAIAAAAAAAAAAABAIdFgBQAAAAAAAAAAAIBCosEKAAAAAAAAAAAAAIVEgxUAAAAAAAAAAAAACokGKwAAAAAAAAAAAAAUEg1WAAAAAAAAAAAAACgkt2ywbtmyRaNGjVJaWpop4y1cuFAPPvigateuLZvNpttvv92Ucb2BmVmcPHlSb775plq3bq3KlSsrODhYzZs318KFC4tfqBcwe78YPHiwmjRpopCQENntdt10000aNWqUzp49a8r4nsrsHC61b98+lSpVSjabTT///LPp43sas7OoWbOmbDZbgZ8nnnjClPE9mTP2izNnzmjo0KEKDw9XYGCgqlevrtjYWJ0/f960dXgiM7P4/vvvL7tP5P2MHTu2+AV7MLP3iwsXLmjcuHGqV6+e7Ha7qlevrvvuu0+7du0yZXxPZXYOZ8+e1aBBg3TdddcpMDBQN910k6ZNm2bK2J7EyuO5jIwMvfjii6pWrZqCgoLUrFkzrVmzxpQ6Shqrcjh79qxGjhypDh06KCQkRDabTZ988okpNZRUVmURFxenZ555RlFRUSpdurSuv/563X///YqPjzeljpLIqix27dql++67T7Vq1ZLdblelSpXUunVrffXVV6bUURK5y2d/Y8eOlc1m080332xKHSWRVVlc7Xhj69atptRSkli9T/zyyy/q0qWL43PCm2++We+8844ptZQ0VmXRt2/fqx6DHz161JR6ShIr94s9e/aoR48euu6662S321W3bl2NGTPGaz+XsjKLbdu2qUOHDipXrpzKli2r9u3ba8eOHabUUVxu22AdPXq0aWFNmzZNy5YtU40aNVShQgVTxvQWZmbx448/6pVXXlFISIiGDx+usWPHym63q0ePHho5cmTxi/VwZu8XcXFxatWqlUaPHq0pU6bojjvu0Pjx49WhQwfl5uaasg5PZHYOlxo8eLD8/PxMH9dTOSOLRo0a6bPPPsv388gjj5g2vqcyO4tTp06pVatW+vjjj9WzZ09NmzZNAwYM0IULF5SRkWHKOjyVmVncdNNNBfaHzz77TO3bt5ckx39xeWbvF7169dKIESN0++2365133lH//v21YcMGtWjRQgcPHjRlHZ7IzBxycnJ05513atq0abr//vs1efJkRUZG6qmnntIbb7xR/GI9iJXHc3379tXbb7+tXr16acqUKfL19VWnTp20adMmU2opSazKITk5WWPGjNHu3bvVsGFDU9Zd0lmVxYQJE7R48WK1bdtWU6ZM0eOPP64NGzaoSZMm2rlzpym1lDRWZXHw4EGdOXNGDz30kKZMmaJXX31VktSlSxfNmDHDlFpKGnf47O/IkSN64403VLp0aVNqKKmszmLAgAEFjjkiIiJMqaUksTKHb7/9Vi1atFBSUpJeffVVTZkyRXfddZeOHDliSi0ljVVZ9O/fv8C+MHv2bNntdtWrV0/Vq1c3pZ6SxKosDh8+rKZNm2rr1q165plnNHnyZLVo0UIjR45Uz549TamlpLEqi19++UXR0dHav3+/Ro4cqREjRmjPnj2KiYnRn3/+aUotxWK4oTfffNOQZPz1119Xfdz69esL9bhDhw4ZOTk5hmEYRlRUlBETE2NOoV7AzCz2799vHDhwIN+y3Nxco02bNkZgYKBx9uxZEyr2XGbvF5czceJEQ5Lx448/Fq1IL+CsHFavXm0EBAQYw4cPNyQZcXFxxS/Ww5mdxQ033GD85z//Ma9AL2J2Fk8++aQRHBxs7N+/37wivYQr3isiIiKM2rVrF61AL2JmFkeOHDEkGUOGDMm3/LvvvjMkGW+//bYJFXsmM3P4/PPPDUnGRx99lG/5vffea5QqVco4fvy4CRV7BquO53766SdDkvHmm286lqWnpxs33nij0aJFi2vZBI9gVQ4XLlwwjh07ZhiGYcTFxRmSjFmzZl1j9Z7Fqiw2b95sZGRk5FsWHx9vBAYGGr169Sps+R7FnT5vys7ONho2bGhERkYW+jmexB2y6N69u9GmTRsjJibGiIqKKmTlnseqLPLG++KLL4pQteexKodTp04ZYWFhxj333ON4vLdzh9enPBs3bjQkGWPHji30czyJVVmMHTvWkGTs3Lkz3/I+ffoYkoyUlJTCboLHsCqLTp06GRUqVDCSk5MdyxISEowyZcoY3bp1u5ZNcAq3O4N11KhReuGFFyRJ4eHhjlPgDxw4UOQxa9SoIR8ft9tUt2d2FuHh4brhhhvyLbPZbLr77ruVkZGh/fv3F7dkj+WM/eJyatasKUlOOTvTEzgrh6ysLA0cOFADBw7UjTfeaEKlns+Z+0RmZqbOnTtX7HG8hdlZpKWladasWXr88ccVHh6uzMxMzlotJFe8V/zvf//T3r171atXL9PG9ERmZ3HmzBlJUlhYWL7lVatWlSQFBQUVvVgPZnYOGzdulCT16NEj3/IePXrowoULWrZsWbHq9RRWHs8tWrRIvr6+evzxxx3LSpUqpUcffVQ//vijDh8+XOQaShorcwgMDFSVKlWKvB5PY2UWLVu2VEBAQL5ltWvXVlRUlHbv3l3k9ZdU7vZ5k6+vr2rUqOGVx9/ukMWGDRu0aNEiTZ48ucjr9ATukIV08e/d7OzsIq+zpLMyh3nz5un48eMaO3asfHx8dO7cOa+e2c5d9ok88+bNk81m0wMPPFDk9ZdUVmZx+vRpSZc/Bvfx8Snw95WnszKLjRs3ql27dqpYsaJjWdWqVRUTE6MVK1ZYfrlDt5uLslu3boqPj9f8+fM1adIkVapUSZJUuXJliyvzPq7KIjExUZIc46MgZ2WRnZ2ttLQ0ZWZmaufOnRo+fLjKli2rpk2bmlG2x3FWDpMnT1ZqaqqGDx+uJUuWmFGqx3NWFt99953sdrtycnJ0ww03aPDgwRo4cKAZJXsss7PYtGmTLly4oIiICMXGxurLL79Ubm6uWrRooffff1+NGjUysXrP4or37blz50oSDdZ/YHYWN954o6677jq99dZbioyMVOPGjZWQkOC4TvHfG364yOwcMjIy5OvrW+Bg2m63S7p4XZjHHnuseEV7ACuP57Zv3646deqoXLly+Zbn/W27Y8cO1ahRw+l1uAOOq92Hu2VhGIaOHz+uqKgoS9ZvJXfI4ty5c0pPT9epU6e0fPlyrVq1St27d3fZ+t2F1Vnk5OTo2WefVb9+/VS/fn2XrNNdWZ2FJD388MM6e/asfH191apVK7355pu69dZbXbZ+d2BlDmvXrlW5cuV09OhR3X333YqPj1fp0qXVu3dvTZo0SaVKlXJ6De7EHfaJPFlZWfr888/VsmVLx0kx3sTKLG6//XZNmDBBjz76qEaPHq2KFStqy5YtjktYedvU8lZmkZGRcdkvldvtdkdPo3nz5k6v40rcrsHaoEEDNWnSRPPnz9fdd9/tlS8e7sIVWaSkpOjDDz9Uq1atHGdhoCBnZfHzzz+rRYsWjtuRkZFavny5QkJCTBnf0zgjh8TERL322muaOHFigQ8DcWXOyKJBgwaKjo5WZGSkTp48qU8++USDBg1SQkKCJkyYUPyiPZTZWezZs0eSNGzYMN14442aPXu2Tp06pdGjR6tNmzbatWsX7xdX4Oz37ZycHC1cuFBNmzb1ymsiXQuzs/D399fixYv1wAMPqEuXLo7lt9xyi7Zs2aLg4ODiFeyhzM4hMjJSOTk52rp1q6Kjox3L885sPXr0aLHG9xRWHs8dO3bssu8RecsSEhJcVovVOK52H+6Wxdy5c3X06FGNGTPG0jqs4A5ZPP/885o+fbokycfHR926ddN7773n8jqsZnUWH3zwgQ4ePKi1a9e6dL3uyMosAgICdO+996pTp06qVKmS/u///k8TJ05Uq1attGXLFjVu3NhltVjNyhz27Nmj7Oxsde3aVY8++qjGjRun77//Xu+++67S0tI0f/58l9XiDqx+fbrUN998o5MnT3rtF5ytzKJDhw567bXX9MYbb2j58uWO5a+88opef/11l9XhLqzMIjIyUlu3blVOTo58fX0lXZx98KeffpJk/XG42zVYr+bUqVPKysrKd1uSUlNTVaZMGcfyUqVK5bsN85mRRW5urnr16qW0tDS9++67zi3YgxUni3r16mnNmjU6d+6ctmzZorVr11p+Wn1JVdQcXnzxRdWqVUv9+vVzXbEerqhZXPoHk3TxW7QdO3bU22+/rWeffVbXXXedkyv3PEXJIu81yGazad26dY7ljRs3dpzF6o1/zBaXGe/b69at0/Hjx/Xyyy87t1gPV9QsKlSooEaNGum+++5T8+bNtXfvXo0bN0733Xef1qxZ43XfLC+uouTwwAMPaMyYMXrkkUf0/vvvq3bt2vr22281depUSVJ6eroLt6BkcvbxXHp6ugIDAwssz9s/yOgijqvdh6uz+OOPP/T000+rRYsWeuihh4o9nidxVRaDBg1SbGysEhIS9PnnnysnJ0eZmZlFL9wDOTuLkydPasSIEXr11Vc5q/8fODuLli1bqmXLlo7bXbp0UWxsrBo0aKBhw4Zp9erVxajeczg7h7Nnz+r8+fN64okn9M4770i6eLZaZmampk+frjFjxqh27drF3ArP4Or37Xnz5snf31/3339/scfyNK7IombNmmrdurXuvfdeVaxYUV9//bXeeOMNValSRc8880zxNsCDODuLp556Sk8++aQeffRRDR06VLm5uXr99dd17NgxSW5wjGf1RWAv50oXzI2JiTEk/ePPQw89dMWxr/VC0t7OmVk89dRThiRj9uzZzt0ID+HMLPLMnTvX8PHxMXbs2OGcjfAAZubw448/Gjabzfjuu+8cy2bNmmVIMuLi4ly0RSWXK/aJ1atXG5KMzz77zDkb4SHMzCJvrIcffrjAesLDw4077rjDyVtTsjlzv+jTp4/h6+trJCYmOncjPISZWaSlpRlhYWHGxIkT8431/fffG5KMqVOnumCLSiaz94kffvjBuP766x33lytXzvj0008NSUbXrl1dtl3uzqrjuaioKKNNmzYFlu/atcuQZHzwwQfF2KqSxx2Oq+Pi4gxJxqxZs4q1LSWdO2Rx7Ngxo1atWkaNGjWMo0ePFm+DSjB3yOJS//73v41//etfRm5u7rVvTAlnVRZPPPGEERERYWRkZORbZ1RUlAlbVTK5237Ro0cPIyAgwMjOzr72jSnBrPz7SZLxww8/5Fv+ww8/GJKMTz/9tJhbVvK4wz5x5swZw263G3fddVfxNqaEsyqL+fPnG0FBQcbhw4fzLe/bt69ht9uN5OTkYm5ZyWPlfvHyyy8b/v7+jrFuvfVW45VXXjEkGUuXLjVl+4qqRJ3B+tZbbyk1NdVx+9dff9WQIUM0Z86cfBccrlatmhXleZXiZjF69GhNnTpV48ePV+/evZ1eryczc7/o1q2bevfurQULFqhhw4ZOqddTFSWHoUOHqlWrVgoPD3dcFDw5OVnSxSnuDh06pOuvv941G+BBzNwn8q7TlpKSYn6hXqAoWeT9/6X35wkNDc03HgqvuPtFenq6li5dqnbt2l02GxReUbJYvHixjh8/nm96YEmKiYlRuXLltHnzZj355JPOL96DFHWfaN26tfbv36/ff/9d586dU8OGDR3TztapU8c1xZdgzj6eq1q16mWniMr7djPHiRdxXO0+XJXFqVOn1LFjR6WlpWnjxo1kexlW7RexsbHq37+/4uPjFRkZaerYJZUzs9izZ49mzJihyZMn55s2/sKFC8rKytKBAwdUrlw5Lpv0/7Nqv6hRo4YyMzN17tw5LqUk5+dQrVo17dq1q8BxXmhoqCRxDH4JV+4TX375pc6fP++10wP/E2dnMXXqVDVu3LjAbHZdunTRJ598ou3bt6tdu3ZFK97DuGK/GDt2rIYMGaJdu3apfPnyql+/vmN2NauPw92ywWqz2S67/JZbbsl328/vYvm33Xab5dcx8VTOyOL999/XqFGjNGjQIL344oum1OkNXLFfZGRkKDc313EqPwoyM4dDhw7p4MGDCg8PL3Bfly5dVL58eaWlpRWrXk/min1i//79klxz0faSzMws8p5zuQ/IExISVLdu3WJU6vmctV8sX75cZ86c4eDuGpiZxfHjxyVdvA7upQzDUE5OjrKzs4tZredyxj7h6+urRo0aOW7nXb+NA+z/x6rjuUaNGmn9+vU6ffp0vg9k867Pc2lu3oDjavdhZRYXLlxQ586dFR8fr7Vr16pevXqmjFtSudt+kTetnTceg1uRxdGjR5Wbm6sBAwZowIABBe4PDw/XwIEDNXny5GKtp6Rxt/1i//79XjlNvVU53HLLLVqzZo2OHj2a74seeV9C8MbPQ9xhn5g7d67KlClT4Eu23saqLI4fP64KFSoUWJ43Fa43HoNbvV9UqFBB0dHRjttr167VddddZ/nnhD6Wrv0KSpcuLUk0FtyA2VksXLhQAwYMUK9evfT222+bMqa3MDOLtLS0fHOj5/nwww8lSbfeemux1+GpzMxhxowZWrp0ab6fZ599VpI0ceJEzZ07t9jr8GRmZpGSklKgcZGVlaXx48crICBAd9xxR7HX4cnMzCIyMlINGzbUsmXLHGd0S9K3336rw4cP69///nex1+HJnPU31Lx582S323XPPfeYOq4nMzOLvG9kLliwIN/y5cuX69y5c2rcuHGx1+GpnH1cceLECU2YMEENGjSgwXoJq47nYmNjlZOToxkzZjiWZWRkaNasWWrWrJljZgpvwXG1+7Aqi5ycHHXv3l0//vijvvjiC7Vo0cKl63dHVmWRlJRUYFlWVpZmz56toKAgr2x8W5HFzTffXOAYfOnSpYqKitL111+vpUuX6tFHH3VZPe7Cqv3ixIkTBZb9+uuvWr58udq3by8fH7f82NpprMoh7/qeH330Ub7lH374ofz8/HT77be7tB53YPXfUCdOnNDatWt1zz33yG63W1KDu7Aqizp16mj79u2Kj4/Pt3z+/Pny8fFRgwYNXFqPO7B6v7jUwoULFRcXp0GDBln+XuGWZ7Dmdb1feeUV9ejRQ/7+/urcubMjxGu1YcMGbdiwQdLFF6hz587p9ddfl3Rxmq/WrVubU7gHMjOL//3vf+rTp48qVqyotm3bFmgetWzZUrVq1TKlbk9kZhbff/+9BgwYoNjYWNWuXVuZmZnauHGjlixZoltvvVUPPvig2eV7DDNzaN++fYFleW9SMTExNLr/gZlZLF++XK+//rpiY2MVHh6ulJQUzZs3Tzt37nRcwB5XZvb79qRJk/Tvf/9b0dHR6t+/v06dOqW3335bderUYRrUf2B2FtLFLyCsWrVK9957r9d9i7w4zMyic+fOioqK0pgxY3Tw4EE1b95ce/fu1XvvvaeqVat65QeBhWX2PhETE6MWLVooIiJCiYmJmjFjhs6ePasVK1ZYfmDnTqw6nmvWrJnuu+8+DRs2TElJSYqIiNCnn36qAwcOFPjQ0BtYeVz93nvvKS0tzXH2y1dffaUjR45Ikp599lmVL1++yNtVElmVxfPPP6/ly5erc+fOSklJ0Zw5c/KN443HfVZl0b9/f50+fVqtW7dW9erVlZiYqLlz5+qPP/7QW2+95ZV/Y1mRRaVKlXT33XcXeG7eGauXu88bWLVfdO/eXUFBQWrZsqVCQ0P1f//3f5oxY4bsdrvGjx9vwpaVLFbl0LhxYz3yyCP6+OOPlZ2drZiYGH3//ff64osvNGzYMK+cVt7q3sTChQuVnZ3NDFKyLosXXnhBq1atUqtWrfTMM8+oYsWKWrFihVatWqV+/fqxX7gwiw0bNmjMmDFq3769KlasqK1bt2rWrFnq0KGDBg4caMKWFZOlV4C9itdee82oXr264ePjc9mL5xqGYaxfv/6K911q5MiRV7y47siRI51SvycxK4tZs2Zd9ULHs2bNcto2eAqzsti7d6/Rp08fo1atWkZQUJBRqlQpIyoqyhg5cqRx9uxZ522AhzDz9env8vaTuLg4c4r1cGZl8fPPPxudO3c2qlevbgQEBBhlypQxoqOjjc8//9x5xXsYs/eLNWvWGM2bNzdKlSplhISEGL179zaOHTtmfuEeyOwsPvjgA0OSsXz5cvOL9XBmZpGSkmIMHjzYqFOnjhEYGGhUqlTJ6NGjh7F//37nFO9BzMxh8ODBRq1atYzAwECjcuXKxgMPPGDs27fPOYWXcFYdz6WnpxtDhgwxqlSpYgQGBhr/+te/jNWrV5u3YSWMVTnccMMNV3zstf597CmsyCImJuaqx+Deyoos5s+fb7Rr184ICwsz/Pz8jAoVKhjt2rUzli1bZu7GlTDu8tlfTEyMERUVVfQN8QBWZDFlyhSjadOmRkhIiOHn52dUrVrVePDBB409e/aYu3EliFX7RGZmpjFq1CjjhhtuMPz9/Y2IiAhj0qRJpm1XSWTl61Pz5s2N0NBQIzs725yNKeGsyuKnn34yOnbsaFSpUsXw9/c36tSpY4wdO9bIysoyb+NKGCuy2Lt3r9G+fXujUqVKRmBgoFG3bl1j3LhxRkZGhrkbV0Q2wzAMAQAAAAAAAAAAAAD+EfNYAQAAAAAAAAAAAEAh0WAFAAAAAAAAAAAAgEKiwQoAAAAAAAAAAAAAhUSDFQAAAAAAAAAAAAAKiQYrAAAAAAAAAAAAABQSDVYAAAAAAAAAAAAAKCQarAAAAAAAAAAAAABQSH6FfaDNZtPEiRPVuHFjZ9bj0bZt26ahQ4fKMIxijUMWxWdWFt99951JFXkvM7Igh+Iza5+QpO3btys1NdWEqrxbmzZtTBknISFBf/zxhyljeYvo6GgFBASYPm5CQoL279+v6OhoSdL+/ft14MAB09fjCUqXLq1mzZpZXQYAAAAAAABwWYVusD766KP6z3/+o7p16zqzHo+Wk5NjyjhkUXxmZWFWA8SbmZEFORSfWfuEJL78UUwJCQlatWqVaeNVq1ZN1apVM208FN3fs6hVq5Zq1aplYUUAAAAAAAAAiqLQUwS/9tprfAhYRElJSTp27JhSUlJMGe/DDz+kuepm0tPTlZSUZHUZXo8c3AdZFN2uXbvUr18/08Yji6LLzMzUsWPHTBuPLNwDObgPsnAfZOE+yMI9kIP7IAv3QRbugyzcAzm4D7JwH2QBb1PoBmu1atW0adMmZ9bisRo0aKBq1aqpR48eVpcCJ5kzZ44aNWpkdRlejxzcB1m4D7Ioug0bNph69i9ZuAdycB9k4T7Iwn2QhXsgB/dBFu6DLNwHWbgHcnAfZOE+yALeptBTBAO4uqSkJEVGRuqbb75RzZo1tXv3bnXr1k1bt25V+fLlrS7PLUydOlUrVqzQypUrnbaOS3NYsmSJpk+f7rR1lWQ33nijU3OQyKKwyMJ9/D0LM65J/HdkUTjO3i/ycsDVufL1CVdHFu6DLNwH7xXugX3CfZCF+yAL98F7hfWeeuopDRw40OnrIYt/Rhbuw1VZnDhxQq1atXLK51uepDjvFTRYAZPk5OQoPj5emZmZkqQLFy4oPj7e1GtblkTnz5/X66+/LknavHmzkpOTnbq+S3NITk5WfHy8U9dXUvn5Of/lnywKx5lZfPTRR1q6dKkji/Hjx+vXX38liysgC/fhrCz+ngOuzpXvFbg6snAfZOE+nJ0FORQO+4T7IAv3QRbug/cK633xxRcyDEODBg1y6nrI4p+RhftwVRbZ2dn6888/nboOT1Cc9woarACcKjMzU6tWrZIkJSYmKiQkxOKK4Gz79u3T4cOHHbd3796txMRECyvyXnPnztX69esdtzlb0jpk4R7+ngOs8/f3CliHLAAAAOAsmzdvVmpqqtMbSfhnZOE+XJHFyZMntWvXLqeNj4tosAJwquDgYG3btk2SNHbsWC1YsMDiiuBsjz32WL4Gxt13321dMQAAt/T39wpYhywAAAAAwLMsWbJEjz/+uNVleDwarACcKiUlRXXq1JFhGEpPT1d4eLjVJQEAAAAAAAAAABSZj9UFAJ5owYIFeumll6wuwy0YhqGUlBSlpKQoPT3d6nIAAAAAAAAAAAB05MgR9ezZU6dOnbrm59JgBZzgt99+07fffmt1GQAAAAAAAAAAALiM06dPa8GCBbpw4cI1P5cGKwAAAAAAAAAAAAAUEg1WAAAAAAAAAAAAACgkGqwAAAAAAAAAAAAAUEg0WAEAAAAAAAAAAACgkGiwAgAAAAAAAAAAAEAh0WAFAAAAAAAAAAAAgEKiwQoAAAAAAAAAAAAAhUSDFQAAAAAAAAAAAAAKiQYrAAAAAAAAAAAAABQSDVYAAAAAAAAAAAAAXsff3182m+2an0eDFQAAAAAAAAAAAIBXiYyM1MmTJ1W5cuVrfq6fE+oBAAAAAAAAAAAAALfl6+ursmXLFum5nMEKAAAAAAAAAAAAAIVEgxUAAAAAAAAAAAAACokGKwAAAAAAAAAAAAAUEg1WAAAAAAAAAAAAACgkP6sLAOBcmzZt0qZNmxy377rrLtntdn355ZcaNGiQfHzyf89i+vTpSk1NlST5+Pho4MCBCgwMdGnNAAAAcI0HHnhAgYGBWr16tdWlAAAAAABQYtBgBZygUqVKuv7663XkyBFL6zh48KAWLFig999/X5JUq1YthYSEqGzZspo1a5Y6duyomjVrKigoSNnZ2frrr780b948/f777zp//rzCw8P11FNP0WDFNalRo4ZCQ0OVlJRkdSkAAOAf9OvXT4Zh0GAFAAAAAOAaMEUw4ATPPfecvvzyS6vLUIcOHRzNVZvNpq1bt2rjxo367LPP9OOPP6phw4aKi4uTJCUmJqpOnTr6+OOP1a9fP9WvX1+7d+9WmTJlilWDYRjF3g6ULJ9++qlef/11q8sAAAAAAAAAAMApOIMVLjFgwAAtXrzYcXvJkiVq1qyZhRU51/jx4/X2228rNzdXUVFRBabhdZVLzyA0DEMNGjTQqVOnlJubqzp16igrK0vdunVTYGCgQkNDdfToUYWFhWn48OEaOnRovrFWrVqlfv36XXMNubm5NFmvQdWqVfXzzz87bv/666/q1KmThRUV38aNG/X555/r3XfftboUAAAAAAAAAACKzaMbrLNmzdLZs2fVv39/DRo0SLm5uZKk4OBgjR8/3uLqzHHq1Cm9+OKLGj16tMLCwqwu54pSU1OVkJDguJ2RkeGyda9cuVJxcXEaOXKky9bZpk0bnT17Vm+88YaGDh0qu93usnVfavTo0Tp27Jjj9pAhQ/TFF1/op59+cizv27evateurdKlS6tatWqSpHLlyhUY66abbtKIESOuuYZz587p+eefL+IWeKaJEyc6csjz/PPPF8hBkvz9/fXBBx9Ikj744APt2LHD1eUWW2hoaLHPhHaWvCzy3isGDx6sQYMGqXbt2vket3jxYq1Zs8aiKr1LVFSUI4ucnByrywEAAAAAAACAAjy2wbpp0ybt2LFD6enpWrdunWbOnKmbb75Z58+f1/nz5z2mwXr+/HlNnz5dzz33nNs2WDdt2qTExMR8y7Zv366qVasWaGLk2bhxo+rWravKlSsXe/2//PKLPv/8c5c2WJs2baqQkBBt2bJF/fr1U9myZV227ktt3bpVBw4ckHRxiuDIyEg1a9ZMQUFBys3N1caNG9WlSxe1bt36imPkZVGzZk3179//mms4ffq0VqxYIcMwHLV4q9KlS6tp06bq16+fduzYoT179ujmm2++ag6VK1dW//79tXHjxss2vlE0l2aRkZGh06dP67HHHtNXX32lPn36qEGDBo7H/vTTTzp48KBSUlK0bds2C6v2bLfddpv8/f3VrFkzRxZZWVlKSEjQwYMH1aJFC23dulUXLlywulSvULduXVWpUsVxe/v27Tp16pSFFXmvkJAQx2sSOViLLAAAAAAAQJ4iN1gzMzPl7+8vm81mZj3FZhiGMjIydM8992jSpEmqWLGiY3rNiRMnavfu3Zo0aZLj8Tk5OfnOkPHz8zNlOtecnBzl5ubK39+/2GOVZIZhqEePHjpx4oT8/PyUnZ0tSRo0aJD+/PNPTZ06VdLFf0++vr7y9fVVTk6OOnfurI8++kj33nuvleVfs4CAAMc+ERERoe+++87SembNmuX4f8MwVKNGDb355puaPHmy0tLSFBYWpqysLGVmZkq6WP+lzMiiXLlyjt/DuHHjtHDhwiJuzbXz8/OTv7+/srKyCmybM2VlZclms8nPz8/xu/X19VXdunUdvws/Pz+1bNlSs2fPdnw5Ijc3V9nZ2QoICHA8z2azycfHR507d9a5c+dM2w5vf20KDw93ZPHyyy87lq9YsUL+/v75sujVq5f69eund955R7fddptVJXs0m82mL7/8UpUqVXIs+/bbbyVJU6dO1bhx47R+/XpFRERo3759VpXpNQICAjRixAj17NnTsaxdu3b64YcfHO/jcA0/Pz9FR0dr2bJlyszMVIcOHbRx40ZysEizZs20cuVKSewTVvPx8cn3dxasQxYAAAAAvFWROomJiYkKDg7W3r17za6n2Hbt2qWQkBAlJycXuK9Tp076448/tGvXLseyV199VcHBwY6f5cuXm1LHm2++qaZNm5oyVklms9m0d+9epaWladGiRVd8XJMmTUr89RmrVKmitLQ0RUREWF3KZeVlcf/992vNmjWqVq2asrKy1KlTJwUHBysqKsrpNbzwwgv5psV1thEjRmjDhg3y8fHRX3/9pbS0NJf8tGnTRr169dKePXsctQwbNkybN2923J4xY4Yef/xxRw6S9OGHHzpyaNKkiYKDg9WmTRvHc2bPnm1aja7MoaQ4f/68KleurM2bN+fLQrr4XnFpFoCnuvS94lIrV67UxIkTLarKe02fPl1ffPGF4/Vp+PDh5OAm2Ces1a5dOyUkJHj9F8bcAVkAAAAA8FZFOoPVMAylp6fLMAzNmjVLCxYsMLuuIjt9+rTS09MlSRMmTMh3tldmZqaWL1+ulJQUzZs3Tw8//LC+//571ahRQ1OmTNG9996rUaNGadq0acWuY//+/S49Y85dGYahPn366Iknnijw+1ixYoXS0tI0b948ffDBB1q9erXuvPNO2Ww2ffbZZ2rWrJlFVReNzWZTUFCQ1WVIktLS0tSjRw8ZhqHevXvrwQcfdGRx6tQpJScnO6bZzPu2+aXTbn744Yf64osvZBiGzp07Z1pdfn5+8vNz3czkfn5+CgwMlCSVKlXKZfn4+vpq3bp1Onz4sGPZ/PnzdeLECcf1VP39/eXn55fv996xY0fVrVtX0sU80tPT9dtvv6ljx46Os1eLug2nT5/W/fffr2nTpik8PLwYW+eZ4uLiNGzYMJ05c0a5ubn5spCk7OxszlJykpo1a2r69OkqX778Ze/v3LmzwsLCdOedd+a7ljfMl5dFlSpV5Ovrm+++gIAAPjx3IZvNpoULFyo6OloBAQHKzs5Wenq6XnzxRZ09e9bq8rxWXFyc7rzzTsftQ4cOWViNd/vll1/UvXt33pvdAFkAAAAA8FbF7nSULVtWVatWNaOWYtuxY4d+/fVXSVL37t0VFxennTt35nuM3W5X2bJl9emnn2rlypVKSkpSzZo1lZiYqJ49e5p2YJiUlOSSaZLsdrseeught702o81mU1hYmIKCgpSRkZHvvsOHD2v16tX69NNP1a1bNy1fvlzbtm1Tly5d1K5dO9OaYQ0aNND58+dNGask2Lt3r1atWqVvvvlGkhxTmuZlsW3bNu3fvz/fcyIjIxUTE+PIIj4+Xtu2bdNdd92lXr166YYbbnD5dpjhl19+cVz/dcGCBSpdurRL1puQkOD4kaSuXbsqODg43/Snf7d69Wp17drVcR3Wbt26aeXKldq1a5fWrFkjSfr++++L/MH6mTNn9O2332ru3LmqUaOGgoOD1bVr1yKNVdI1atRI//nPf/ItS05O1rp16yT9vyxuvfVWffrppzpz5owVZXqNMmXKqH379o7bR44ccWSRJzU11TFlMJznn7LgzHfXMQxDx48fd/y7z8jIUG5urn7++WeLK/NuycnJvBa5iUvft2EtsgAAAADgrYrUYPX19VVoaKh8fX0VGxur2NhYs+sqkv/+9786duyYbDabpk6dqoEDBxZoJLVo0UJPPvmkGjduLOnih4nnz5/Xyy+/rD///FNly5Y1pZZJkyZpyZIlpox1NeXLl9cnn3zi9PUURXZ2tlJSUvTOO+/ozJkz+vrrrws8JjU1VX379lV0dLTKlCmjW265RR9//LGpdXTp0kVdunQxdczLycnJUVJSkipWrChfX1/H9leuXNml1yretGmTBgwYcNn7Ro4cqcTExAL7RZs2bfTyyy+rRo0a+bIw49+WYRg6ceKE47afn59CQkKKPW5hLFq0SOPGjZMkPf300y5Z5+UMGTJEderUuerZu+PGjVNaWpr+9a9/KTk5WUOGDJHNZlNCQoJSU1NVuXJlzZw5U++9916xann11VclSfXq1fPaBus999yjESNGXPH+cePG6dixYxo4cKD69u3rusIgSfr999/5vVskOztbSUlJjtsbN24kCws9++yzVpcAAAAAAACAKyjSNVhDQ0N1/Phx3XjjjWbXUyxDhw7V8ePHlZiYeMUGzieffOJorkoXr9F4/PhxJSQkmNZclaTBgwdr48aNpo1XEu3evVtVq1bVyZMn9fTTT+uBBx646uNHjBjhOOuyJEpKSlJYWJj27dsn6WKToFq1akpNTbW4sosMw1C9evWuei3cPGZmkZKSoqpVqyosLExhYWFq1aqVKeOWJK1atVJYWJjuuOOOf3xsQkKCqlSporCwMFWqVEnffPONfH19FR8fr7vuussF1UIq+F4BeIM//vjD8VodFhb2j+/bAAAAAAAAgLdy3cUQ3UhYWJi+++47x//DeQzDkHTxerjR0dF64oknHPf16NHDcTbd9ddfb0l9zjJ58mRNmTJFOTk5atmyZYFr2TnLzJkzCyx77733tHDhQknSV199pbFjx+qrr74q8LgqVapo165dl81i0KBBjmlqr1V2drZyc3OL9FxPk5ubK8Mw1KpVK6Wmpl52ut8qVapo586d6tChg2NZTk6OmjdvzvUnTfLee+9p+/btWrp0qSTppZde0vz58y2uyrv9034BAAAAAAAAAO7EqxqsDz/8sGrWrKly5cqpXr16Vpfj8cLCwjR69GjZ7XZVqlRJbdu21fDhwzVu3Dj16dNHsbGxHptDUlKSDhw4IEn6888/XbbeyzUmTpw4oRMnTshms+nGG2/UI488ovLly2vOnDn5Hufn51cgD8MwNH78eH3zzTf6448/nFq7N/nzzz+VnJx82fvycggICNC3336r+Ph4x3NgjhMnTmjv3r2SLn754+uvv9ahQ4cKPK5cuXJ6/vnnNXnyZLc5E90TJSUlaeTIkdq+fbtXXS8bAAAAAAAAQMnl0Q3W6667TjfffLNyc3O1e/du9e3bV61bt7a6LK8RGhrqOENVkiIiIjRs2DBNmDBBMTExTL/pBAcOHFBmZqbq1q172Ybo7t27FRERodtuu61Ag/VSiYmJSk5OVm5urt544w3OKDPJhQsXtHPnTuXk5BTq8WvXrnVyRd7rwoUL+v333zVhwoQCzdOaNWuqTJkyCgsL04gRIzR79mwarE6UlJSk11577Yr3h4SEqHLlynzJwA2QBQAAAAAAAHBRka7BWlKMGzdOv//+u+Li4hQQECCbzWZ1SV4vb6rYvn37XvUDdRRN//79tW3bNv3www8F7jMMQzExMapfv76efPLJy+4PeVM6v/3226pfv74aNmxIc9VEe/fuVcOGDZWWlnbVx+XlAOfZu3evGjRocNnG6SeffKLff/9da9euJQsL2Gw2x+uTzWZT9+7d+bKBhfLyIAtrXbpfAAAAAAAAwHoe3WDNY7fbdeTIETVv3tzqUrzaunXrVLNmTWVlZVldiley2WzavXu3YmNj1bZtWx04cED+/v6O+48dO6bQ0FDH1MYwX926dZWUlKSkpCTFxsZe9jF5Ofz1118urg55unbtqhdffJEsLDB16lTHFOv+/v5atmyZ/vvf/1pdltdq27at4zUrKSmJLCxyaQ78LQsAAAAAAOAePHqK4DwZGRl64YUX9NJLLykyMtLqcrzWTTfdpAkTJqh///566aWX1KVLF8d9zz33nNq1a6dOnTpZWKF57r//ftntdo0YMULTpk1T6dKlXbLeCRMmXPX+ihUrqlSpUgoICFClSpUkSa+88oq6dOmi8uXLa9KkSRo3bpw2bdok6WJTdvr06QoKCjKlvi+//FK7d+82ZaySKDExUc8995zef/99lSpVqsD969atc0zPXJKNGzdOy5Ytc9x+4YUXtG/fPgsrurKpU6dq7ty52rx5s2PZqVOndObMGeXk5JT4LEqauXPnasuWLcrOzlZOTo7effddpaSkqG3btlaX5pV2796tl156SdOnT5evr68k/eMZ+DDfpTkEBARYXQ4AAAAAAADkJQ1WSTp58iRnTlqsWrVqevDBB7Vs2TL16NFDN998s+O+tLQ0paenW1iduRo1aiTDMDRy5Ejdd999CgkJccl6P/vsM0lSQECAOnbsWOD+y30wW758eZUpU0Z2u10PPvigRo0apX379ql8+fKKjo5Wr169ZLfbTanvwIEDXttgjYiIUO3atZWcnHzFaWfj4+MVHx/v4srMt2bNGv3000+O28uXL7ewmqurUKHCZfeLQ4cOaf369RZU5N02b96cr9m9Zs0aBQQEyMfHKybccDsJCQmaP3++pk2bJl9fXx06dOiyU9DDuRISEjRnzhx17dpVKSkpVpcDD/S///1PO3futLoMAAAAAABKFK9osAYGBrr1B/ze5EpZfPzxxxZU41w+Pj6y2+2WXDMtODhYK1euvOx9/v7+CgwMdNx+9dVXlZCQoEmTJkmSgoKCZLfbVb9+fa1YscIl9XqD3r17a8SIEZKk9PR0ZWdnW1wRJKlnz56XXf7111/r66+/dnE1uByycA8ZGRlasmSJBg8ebHUpXikjIyPfzB+AmUaNGqV169ZZXQYAAAAAACWKVzRYASs0aNBAKSkp+a5z6g5mzpwpwzCUmZkpSVq5cqViYmIc9//yyy+SZElj2BsYhqGIiAglJiZaXQoAFFrHjh21YcMGq8sA4ATLly/XzJkz9dRTT1ldCgAAAAAAJQYNVsAJZs6cqTlz5shms+mrr75S2bJlXbLeiRMn5js79XLyrqPn4+Oj7777TvXr13csk2R6Q/jUqVPq2rWrDMPQwYMHXXY9Wnfz8ccfa926dcrNzdWJEyc0bNgwlS9fXkOHDs33uMqVK2vRokXq2bOnunbtqkaNGql///4WVe3dLs0iISHB6nIAS1y4cEFt2rTR77//rpycHKvLAeAEL7zwArOWAAAAAABwjWiwAk7w119/acOGDfLx8XHptX/r169f6Mf6+PgoOjraidVclJ2drQ0bNjiuO1qvXj2nr9MdHTx4UAcPHnTcrlu3ripWrOi4fffddyslJUW//fabdu7cqczMTIWHh6tx48ZWlAtdnNK8devWCgoKsroUwDK5ubnatGmT1WUAcKJff/1Ve/futboMAAAAAABKFB+rCwAAb1WqVClVq1ZNkjR48GA9+uijKlWqlJ5++mn5+fkpNzdXx48ft7jKaxcaGqrg4GCry7gqf39/Va9eXT4+Bd8GK1eurOuuu05VqlSxoDIAAAAAAAAAgLujwQoAFrnjjjv0559/OqZl7tOnj+Li4iRJmzZt0smTJ9W5c2crSyySBQsW6L///a/VZVxVZGSkDh8+rJCQkAL3LVq0SIcPH3ZkAQAAAAAAAADApWiwAoAFnn/+eT3yyCOXPYNSktq0aaOZM2e6uCpz9O7dW6+88orVZVxVfHy8ateurZSUFL311lv66KOP8t0/Z84cRUREKCIiIt/UznCt+fPna/To0VaXAQAAAAAAAMAD7dmzRxEREUpOTr7m53INVgCwQFJSktavX69XXnlFOTk5mjZtmlasWKEzZ85Ikg4dOmRxhUXXuXNnpaena/HixVaXckWZmZnat2+fJGnt2rWy2+2O+6ZNm6bk5GTH/bDOsmXLivTHDQAAAAAAAAD8k6ysLO3bt085OTnX/FwarABgkQMHDmjy5MmSLk6r6ykaNmyoHTt2WF1Goa1atSrfbU/KoqQjCwAAAAAAAADuiAYrAMBUTz75pNavX291GQAAAAAAAAAAOAXXYAUAAAAAAAAAAACAQqLBCgAAAAAAAAAAAACFRIMVAAAAAAAAAAAAAAqJBisAAAAAAAAAAAAAFBINVgAAAAAAAAAAAAAoJBqsAAAAAAAAAAAAAFBINFgBAAAAAAAAAAAAoJBosAIAAAAAAAAAAABAIdFgBQAAAAAAAAAAAIBCosEKAAAAAAAAAAAAAIVEgxUAAAAAAAAAAAAACokGKwAAAAAAAAAAAAAUEg1WAAAAAAAAAAAAAF4nICBANpvtmp9HgxUAAAAAAAAAAACAV6lbt65SU1MVGhp6zc+lwQoAAAAAAAAAAADAq/j4+MhutxftuSbXAgAAAAAAAAAAAAAey8/qAgBPZhiGUlJSZBiG1aVY5uTJk1aXAAAAgCsIDg5W6dKlde7cOatLAQAAAACPFxQUpODgYKeuo1SpUqpQoYJSU1Oduh5vR4MVcCLDMFSnTh2ry7CcNzeYAQAA3NnixYs1c+ZM9e/f3+pSAAAAAMDjPf/88xozZoxT19G7d2+1bdtW1atXd+p6vB1TBANOZhiG1/8AAABc6qOPPtLIkSOtLgOSbDabbDab1WUAAADAAz3++ONaunSp1WVAZOFuXHEMFhoaqt9++02//fab+vXr5/T1lTSLFi3Sb7/9Vqz9gjNYAZOUKVNGgwYNUsWKFdWmTRv5+vpaXZJbCg0Nder45FA4zsyhd+/estvt+uGHHzRo0CCnrcdTkIX7IAv34awsyOHaOHOfCA8PV/Xq1R3v23kSExP10Ucf6bnnnlNQUJDT1l/SOPvvJ0lkUUhk4T5cdVyRhxwuz4p9ApdHFu7D2Vk0adJEw4cPd+o6PIUzsyCHwrn99tudPrsfWRQOWbiP22+/3SXr8fPzU/369SVJ3bt3V5UqVVyy3pIiOjpaYWFhxRqjSA3W7Oxs7d27V7Vq1VJAQECxCkDxkIV7CA4OVpMmTfTaa69Jktq1a6d27dpZXJX3IQf38PDDD6tMmTI6c+aMIwtYgyyKp3Tp0qpbt64pY5GFeyAH9/L3921J2r17t7Zs2aJXX31V5cuXt7A670IW7oMs3AM5uI/LZQFrkIX7uOWWW3TLLbdYXYbXIwf3QRbugyzcF5+TO4fNKOT8nTabTevWrVObNm107NgxVatWTX/88YciIyOdXWOJZhiGqlatquPHj+dbZhayKJo1a9aoffv2TF/rBsjCPZCD+yALAAAAAAAAAHBv13QGa2xsrAICApSbmytJuu222+TnxyzD/+TEiROmjnfpqdxkUTSZmZmmjRUbG6tNmzaZNp43sNls+u233zR27FjNnj3btHHnzJmjIUOGSJI2b96shQsX6p133jFtfE+wfv163XTTTVaXAQAAAAAAAABAiXVNHbnU1FTFxMTo7rvv1uDBg3Xy5Eln1YWrOH78uF544QXVq1fP6lJKrF27dmnixImmjPXYY4/prrvuMmUsb1K2bFnFxsbK39/ftCyaNm2q8ePHS5IqV66sDh06qFq1aqaM7SmYa9/9RUVFadasWVaXAQAAAAAAAAC4gms+5dFut6tq1arOqAXXoHLlyqpevbrVZZRYl07ZXFyVKlWSj4+PaeN5i40bN0q6+PszS506dfJdrL1JkyZq0qSJaeMDrlCqVCle3wEAAAAAAADAjV3TNVhhjuJeV48szGPGNQ7btGmj9evXm1CNd+N6k9biup/ugywAAAAAAAAAwL0VusEKAAAAAAAAAAAAAN6OeU0BAAAAAAAAAAAAoJBosAIAAAAAAAAAAABAIdFgBQAAAAAAAAAAAIBCosEKAAAAAAAAAAAAAIVEgxUAAAAAAAAAAAAACokGKwAAAAAAAAAAAAAUEg1WAAAAAAAAAAAAACgkGqwAAAAAAAAAAAAAUEg0WAEAAAAAAAAAAACgkP4/pCtcmIOdqSQAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}