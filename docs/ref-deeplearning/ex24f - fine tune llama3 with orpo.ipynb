{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjNdEQfFZsie"
      },
      "source": [
        "# Fine-tune Llama 3 with ORPO\n",
        "\n",
        "> 🗣️ [Large Language Model Course](https://github.com/mlabonne/llm-course)\n",
        "\n",
        "❤️ Created by [@maximelabonne](https://twitter.com/maximelabonne).\n",
        "\n",
        "You can run this notebook on Google Colab (I use an L4 GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4tKGKGGQWBH"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq -U transformers datasets accelerate peft trl bitsandbytes wandb --progress-bar off"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mvo1AYMjgwn3"
      },
      "source": [
        "- For `wandb` API Key, go to the site`https://wandb.ai/` and add `authorize` to the URL\n",
        "- For `huggingface` token, to go [billing](https://huggingface.co/settings/billing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT-pOgixmw0A"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwNxh0pCQhfI"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import wandb\n",
        "from datasets import load_dataset\n",
        "from google.colab import userdata\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
        "\n",
        "# Model\n",
        "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
        "new_model = \"OrpoLlama-3-8B\"\n",
        "\n",
        "# Defined in the secrets tab in Google Colab\n",
        "wb_token = userdata.get('wandb')\n",
        "wandb.login(key=wb_token)\n",
        "\n",
        "# Set torch dtype and attention implementation\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    !pip install -qqq flash-attn\n",
        "    torch_dtype = torch.bfloat16\n",
        "    attn_implementation = \"flash_attention_2\"\n",
        "else:\n",
        "    torch_dtype = torch.float16\n",
        "    attn_implementation = \"eager\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOtlKV5AmtwO"
      },
      "source": [
        "## QLoRA Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2kGBNStQoUd"
      },
      "outputs": [],
      "source": [
        "# QLoRA config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=attn_implementation\n",
        ")\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdMblV8Dmry8"
      },
      "source": [
        "## Acquire Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYbeicUVhZoL"
      },
      "source": [
        "To see the model, please check out [here](https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaTou6EQQnAK"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
        "dataset = load_dataset(dataset_name, split=\"all\")\n",
        "dataset = dataset.shuffle(seed=42).select(range(1000)) # Only use 1000 samples for quick demo\n",
        "\n",
        "def format_chat_template(row):\n",
        "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
        "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "    format_chat_template,\n",
        "    num_proc= os.cpu_count(),\n",
        ")\n",
        "dataset = dataset.train_test_split(test_size=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLz3N8sMGVjx"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWDwJe7_Qqgb"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "orpo_args = ORPOConfig(\n",
        "    learning_rate=8e-6,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    max_length=1024,\n",
        "    max_prompt_length=512,\n",
        "    beta=0.1,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=0.2,\n",
        "    logging_steps=1,\n",
        "    warmup_steps=10,\n",
        "    report_to=\"wandb\",\n",
        "    output_dir=\"./results/\",\n",
        ")\n",
        "\n",
        "trainer = ORPOTrainer(\n",
        "    model=model,\n",
        "    args=orpo_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model(new_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPjqZFXAJ2eB"
      },
      "source": [
        "## Push to HF Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6b3dCS6fL45"
      },
      "outputs": [],
      "source": [
        "# Flush memory\n",
        "del trainer, model\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Reload tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "fp16_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "fp16_model, tokenizer = setup_chat_format(fp16_model, tokenizer)\n",
        "\n",
        "# Merge adapter with base model\n",
        "model = PeftModel.from_pretrained(fp16_model, new_model)\n",
        "model = model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvWUrbyNgEnX"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VJ2Efp3mkKu"
      },
      "source": [
        "## Inference (from HF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO2eFCtVmhGh"
      },
      "outputs": [],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"eagle0504/OrpoLlama-3-8B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XHksdDUtmhsh"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "prompt = \"What is large language model?\"\n",
        "outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6NirMovrjiB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}