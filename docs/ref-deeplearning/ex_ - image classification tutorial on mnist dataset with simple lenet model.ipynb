{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "# MNIST Image Classification with TensorFlow\n",
        "\n",
        "This repository contains two scripts, `run.py` and `run2.py`, for image classification using the MNIST dataset with TensorFlow. The scripts demonstrate two different approaches to training a neural network for this task.\n",
        "\n",
        "## Summary\n",
        "\n",
        "### run.py\n",
        "\n",
        "The `run.py` script leverages TensorFlow's high-level API to perform image classification on the MNIST dataset. It first loads and preprocesses the dataset by rescaling the image pixel values and one-hot encoding the labels. The model architecture is defined using TensorFlow's Functional API, comprising three dense layers. The model is then compiled with the Adam optimizer and categorical cross-entropy loss function. Finally, the model is trained using the `model.fit` method, which handles the training loop internally and includes validation on the test set.\n",
        "\n",
        "### run2.py\n",
        "\n",
        "The `run2.py` script implements a manual training loop using TensorFlow's lower-level API. Similar to `run.py`, it loads and preprocesses the MNIST dataset. The model architecture is also defined using the Functional API with three dense layers. However, instead of using `model.compile` and `model.fit`, the script defines a custom training loop using `tf.GradientTape` to manually compute gradients and update the model's weights. The script also includes a validation step at the end of each epoch to evaluate model performance on the test set.\n",
        "\n",
        "## Differences\n",
        "\n",
        "The primary difference between `run.py` and `run2.py` lies in how the model training process is handled. `run.py` utilizes TensorFlow's high-level API methods (`model.compile` and `model.fit`), which abstract away the details of the training loop, making the code more concise and easier to understand. On the other hand, `run2.py` bypasses these high-level methods in favor of a handcrafted training loop using `tf.GradientTape`. This approach provides more control over the training process, allowing for custom training logic, but it also requires more code and a deeper understanding of TensorFlow's lower-level operations.\n",
        "\n",
        "By comparing these two scripts, users can gain insight into both high-level and low-level approaches to training neural networks with TensorFlow, each with its own advantages and trade-offs.\n"
      ],
      "metadata": {
        "id": "-oZDS3RzKdMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `run.py`"
      ],
      "metadata": {
        "id": "nEpGRorRKf9-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1kuqE38JD8a",
        "outputId": "95e46e0c-8b68-463b-a1b4-0f6db531c770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 16s 6ms/step - loss: 0.2416 - accuracy: 0.9278 - val_loss: 0.1229 - val_accuracy: 0.9622\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1024 - accuracy: 0.9695 - val_loss: 0.1125 - val_accuracy: 0.9656\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0726 - accuracy: 0.9771 - val_loss: 0.0863 - val_accuracy: 0.9737\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0554 - accuracy: 0.9826 - val_loss: 0.0831 - val_accuracy: 0.9744\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0421 - accuracy: 0.9865 - val_loss: 0.1040 - val_accuracy: 0.9715\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0372 - accuracy: 0.9879 - val_loss: 0.0788 - val_accuracy: 0.9781\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0293 - accuracy: 0.9903 - val_loss: 0.0920 - val_accuracy: 0.9745\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0229 - accuracy: 0.9926 - val_loss: 0.0920 - val_accuracy: 0.9769\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.0993 - val_accuracy: 0.9763\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0186 - accuracy: 0.9936 - val_loss: 0.0957 - val_accuracy: 0.9762\n",
            "CPU times: user 1min 36s, sys: 7.67 s, total: 1min 44s\n",
            "Wall time: 1min 48s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79474024ee60>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten\n",
        "\n",
        "# Load MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Rescale data\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build model using Functional API\n",
        "inputs = Input(shape=(28, 28))\n",
        "x = Flatten()(inputs)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We ran the code using a CPU and the wall time is: `Wall time: 1min 48s`."
      ],
      "metadata": {
        "id": "w3idPIMlLjge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `run2.py`"
      ],
      "metadata": {
        "id": "7BfWIx39Khfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only thing different here is that the following code uses `gradienttape` from `tensorflow` library and this allows us to use a `for` loop so that the training process is more transparent. For example, we can see inside of the for loop the code computes the `loss` and update the `gradients`, which are required steps to train any neural network models."
      ],
      "metadata": {
        "id": "N80_kRJTLrAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "# Load MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Rescale data\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build model using Functional API\n",
        "inputs = Input(shape=(28, 28))\n",
        "x = Flatten()(inputs)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = Adam()\n",
        "\n",
        "# Training parameters\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=1024).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    train_loss = tf.keras.metrics.Mean()\n",
        "    train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "    # Training step\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch_train, training=True)\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        train_loss.update_state(loss_value)\n",
        "        train_accuracy.update_state(y_batch_train, logits)\n",
        "\n",
        "        if step % 500 == 0:\n",
        "            print(f'Step {step}: loss = {loss_value.numpy()}')\n",
        "\n",
        "    # Validation step\n",
        "    val_loss = tf.keras.metrics.Mean()\n",
        "    val_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "    for x_batch_test, y_batch_test in test_dataset:\n",
        "        test_logits = model(x_batch_test, training=False)\n",
        "        val_loss.update_state(loss_fn(y_batch_test, test_logits))\n",
        "        val_accuracy.update_state(y_batch_test, test_logits)\n",
        "\n",
        "    print(f'Epoch {epoch + 1} - Loss: {train_loss.result().numpy()}, Accuracy: {train_accuracy.result().numpy()}, '\n",
        "          f'Val Loss: {val_loss.result().numpy()}, Val Accuracy: {val_accuracy.result().numpy()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeWbVvDLJw6t",
        "outputId": "44231064-4df5-4e5d-f9fc-8f285046e4b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Step 0: loss = 2.3509457111358643\n",
            "Step 500: loss = 0.4965081810951233\n",
            "Step 1000: loss = 0.3434571325778961\n",
            "Step 1500: loss = 0.06301655620336533\n",
            "Epoch 1 - Loss: 0.23892655968666077, Accuracy: 0.930400013923645, Val Loss: 0.12120147794485092, Val Accuracy: 0.9617999792098999\n",
            "Epoch 2/10\n",
            "Step 0: loss = 0.08149765431880951\n",
            "Step 500: loss = 0.07721756398677826\n",
            "Step 1000: loss = 0.059084922075271606\n",
            "Step 1500: loss = 0.11262740939855576\n",
            "Epoch 2 - Loss: 0.09931076318025589, Accuracy: 0.9697999954223633, Val Loss: 0.10143530368804932, Val Accuracy: 0.9677000045776367\n",
            "Epoch 3/10\n",
            "Step 0: loss = 0.13360005617141724\n",
            "Step 500: loss = 0.008813761174678802\n",
            "Step 1000: loss = 0.0466441847383976\n",
            "Step 1500: loss = 0.008642111904919147\n",
            "Epoch 3 - Loss: 0.06859397143125534, Accuracy: 0.9794333577156067, Val Loss: 0.09209497272968292, Val Accuracy: 0.9732000231742859\n",
            "Epoch 4/10\n",
            "Step 0: loss = 0.07262778282165527\n",
            "Step 500: loss = 0.12308397889137268\n",
            "Step 1000: loss = 0.016974708065390587\n",
            "Step 1500: loss = 0.006350511685013771\n",
            "Epoch 4 - Loss: 0.05025269463658333, Accuracy: 0.9842833280563354, Val Loss: 0.0998232439160347, Val Accuracy: 0.9728000164031982\n",
            "Epoch 5/10\n",
            "Step 0: loss = 0.2687130570411682\n",
            "Step 500: loss = 0.0194102693349123\n",
            "Step 1000: loss = 0.11595699191093445\n",
            "Step 1500: loss = 0.011517774313688278\n",
            "Epoch 5 - Loss: 0.0402318499982357, Accuracy: 0.9865166544914246, Val Loss: 0.09719943255186081, Val Accuracy: 0.9742000102996826\n",
            "Epoch 6/10\n",
            "Step 0: loss = 0.27532610297203064\n",
            "Step 500: loss = 0.015855444595217705\n",
            "Step 1000: loss = 0.016766168177127838\n",
            "Step 1500: loss = 0.03971192240715027\n",
            "Epoch 6 - Loss: 0.03263767063617706, Accuracy: 0.9897333383560181, Val Loss: 0.10093387961387634, Val Accuracy: 0.9745000004768372\n",
            "Epoch 7/10\n",
            "Step 0: loss = 0.07816750556230545\n",
            "Step 500: loss = 0.0004550150188151747\n",
            "Step 1000: loss = 0.006819138303399086\n",
            "Step 1500: loss = 0.008547327481210232\n",
            "Epoch 7 - Loss: 0.025356443598866463, Accuracy: 0.9918500185012817, Val Loss: 0.10509855300188065, Val Accuracy: 0.9745000004768372\n",
            "Epoch 8/10\n",
            "Step 0: loss = 0.046413570642471313\n",
            "Step 500: loss = 0.007510469760745764\n",
            "Step 1000: loss = 0.002416808856651187\n",
            "Step 1500: loss = 0.03213512897491455\n",
            "Epoch 8 - Loss: 0.022631967440247536, Accuracy: 0.9922500252723694, Val Loss: 0.11049787700176239, Val Accuracy: 0.9743000268936157\n",
            "Epoch 9/10\n",
            "Step 0: loss = 0.0030062589794397354\n",
            "Step 500: loss = 0.004442516248673201\n",
            "Step 1000: loss = 0.000896106066647917\n",
            "Step 1500: loss = 0.0003119784814771265\n",
            "Epoch 9 - Loss: 0.020402774214744568, Accuracy: 0.9931333065032959, Val Loss: 0.09815893322229385, Val Accuracy: 0.9779999852180481\n",
            "Epoch 10/10\n",
            "Step 0: loss = 0.005459990352392197\n",
            "Step 500: loss = 0.0005269083776511252\n",
            "Step 1000: loss = 0.002629747847095132\n",
            "Step 1500: loss = 0.0008299302426166832\n",
            "Epoch 10 - Loss: 0.019028134644031525, Accuracy: 0.993316650390625, Val Loss: 0.11351626366376877, Val Accuracy: 0.9750999808311462\n",
            "CPU times: user 9min 21s, sys: 8.21 s, total: 9min 29s\n",
            "Wall time: 9min 54s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "On5v7BWjKWXV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}