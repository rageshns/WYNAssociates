{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNm39gNlliHrKIwKBxFApA7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z6VQIukZ3gEc","executionInfo":{"status":"ok","timestamp":1711508710721,"user_tz":240,"elapsed":15285,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"138ec81d-9008-4696-8e40-33df57c29250"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai\n","  Downloading openai-1.14.3-py3-none-any.whl (262 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/262.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/262.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.9/262.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n","Installing collected packages: h11, httpcore, httpx, openai\n","Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.3\n"]}],"source":["! pip install openai"]},{"cell_type":"code","source":["key = \"sk-your-api-key-here\""],"metadata":{"id":"dkYHOhwK3hFk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from openai import OpenAI\n","client = OpenAI(api_key=key)"],"metadata":{"id":"JX91d4Ia3iBx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = client.chat.completions.create(\n","  model=\"gpt-3.5-turbo\",\n","  messages=[\n","    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n","  ]\n",")"],"metadata":{"id":"2Nit6Onb3xx5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response.choices[0].message.content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"ZVyYYI_Y3y3G","executionInfo":{"status":"ok","timestamp":1711514921450,"user_tz":240,"elapsed":4,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"1c9508ad-6e3f-4b71-adcd-de1cfca83785"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"I'm not sure what you are referring to. Could you please provide more context or clarify your question?\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["def call_chatgpt(question: str) -> str:\n","    \"\"\"\n","    Calls the GPT model using the provided client to generate a response to a given question.\n","\n","    The function takes a question as input and interacts with an OpenAI GPT model through the client's chat\n","    completion API. If there is any exception during the API call, it catches the exception and provides a\n","    default error response.\n","\n","    Parameters:\n","    question (str): The user's question that needs to be sent to the GPT model.\n","\n","    Returns:\n","    str: The generated response from the GPT model. In case of an error, a default error message is returned.\n","    \"\"\"\n","    try:\n","        # Generate a response using the GPT model specified, with a fixed system role message followed by the user's question.\n","        response = client.chat.completions.create(\n","            model=\"gpt-3.5-turbo\",\n","            messages=[\n","                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","                {\"role\": \"user\", \"content\": question},\n","            ],\n","        )\n","        # Extract the content of the message received from the GPT model.\n","        output = response.choices[0].message.content\n","    except Exception as e:\n","        # Print the exception and return a default error message if any exception occurs.\n","        print(e)\n","        output = \"Sorry, I couldn't get an answer for that.\"\n","\n","    return output"],"metadata":{"id":"KG1LwBVYPR-u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["call_chatgpt(\"tell me a joke\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"qiAVVPXsPf1w","executionInfo":{"status":"ok","timestamp":1711515168457,"user_tz":240,"elapsed":928,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"1d3dfc07-8a2c-4779-b39b-fbc21d3136fa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Sure, here's a joke for you: Why did the tomato turn red? Because it saw the salad dressing!\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":[],"metadata":{"id":"OJdTB3QKPgl7"},"execution_count":null,"outputs":[]}]}