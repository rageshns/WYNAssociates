{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWxnjLYkrLmc"
      },
      "source": [
        "# L4-D - Building your own Quantizer: Load your Quantized Weights from Hugging Face Hub\n",
        "\n",
        "In this lesson, you will learn memory efficient model loading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSabZvmbrL3Z"
      },
      "source": [
        "Run the next cell to import all of the functions you have used before in the previous lesson(s) of `Building your own Quantizer` to follow along with the video.\n",
        "\n",
        "- To access the `helper.py` file, you can click `File --> Open...`, on the top left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5lO0hiSrMzc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from helper import W8A16LinearLayer, replace_linear_with_target_and_quantize, replace_linear_with_target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8oGnthirNdZ"
      },
      "source": [
        "## Memory Efficient Model Loading\n",
        "\n",
        "- Load [facebook/opt-125m](https://huggingface.co/facebook/opt-125m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_iSdc1UrOcW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"./models/facebook/opt-125m\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zexn4txnrPVr"
      },
      "outputs": [],
      "source": [
        "replace_linear_with_target_and_quantize(model,\n",
        "                             W8A16LinearLayer,\n",
        "                                   [\"lm_head\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFdSmCkQrQkm"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n676GVrYrRax"
      },
      "outputs": [],
      "source": [
        "quantized_state_dict = model.state_dict()\n",
        "torch.save(quantized_state_dict, \"quantized_state_dict.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCu16TuTrSdS"
      },
      "source": [
        "- The below code is for demonstration purposes only.\n",
        "- You'll need your own Hugging Face username in order for it to run.\n",
        "- You'll add your usernmae in `YOUR_HF_USERNAME = \"\"`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLvTkgHArTHD"
      },
      "source": [
        "```Python\n",
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "YOUR_HF_USERNAME = \"\"\n",
        "your_repo_id = f\"{YOUR_HF_USERNAME}/opt-125m-quantized-dlai\"\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# create_repo(your_repo_id)\n",
        "\n",
        "api.upload_file(\n",
        " path_or_fileobj=\"quantized_state_dict.pth\",\n",
        " path_in_repo=\"quantized_state_dict.pth\",\n",
        " repo_id=your_repo_id\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "konCiqUPrVFz"
      },
      "source": [
        "### Load the Model in the Meta Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a33YVMRrWbR"
      },
      "outputs": [],
      "source": [
        "from transformers import OPTForCausalLM, AutoTokenizer, AutoConfig\n",
        "\n",
        "model_id = \"./models/facebook/opt-125m\"\n",
        "config = AutoConfig.from_pretrained(model_id)\n",
        "\n",
        "with torch.device(\"meta\"):\n",
        "  model = OPTForCausalLM(config)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcW-rorkrXOx"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzbUoQ9DrYsR"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXY2UiLprZoK"
      },
      "outputs": [],
      "source": [
        "replace_linear_with_target(model, W8A16LinearLayer, [\"lm_head\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anyI1liKra38"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZfCFK7grb3i"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "state_dict_cache_path = hf_hub_download(\n",
        "    \"ybelkada/opt-125m-quantized-dlai\",\n",
        "    \"quantized_state_dict.pth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Myk4gWoCrcoR"
      },
      "outputs": [],
      "source": [
        "state_dict = torch.load(state_dict_cache_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYW0uYVVrdYe"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(state_dict, strict=True, assign=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q4J3CoDreQW"
      },
      "source": [
        "- Test your model.\n",
        "- **Note:** Your generated text might be different than what you see in the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfr5-kd9re3J"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "pipe(\"Hello today I am\", max_new_tokens=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V88vBHLKrgyZ"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "pipe(\"Hello today I am giving a course about\", max_new_tokens=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
