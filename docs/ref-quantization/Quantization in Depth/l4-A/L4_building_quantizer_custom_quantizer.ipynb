{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# L4-A - Building your own Quantizer: Custom Build an 8-Bit Quantizer\n",
        "\n",
        "In this lesson, you will learn how to compress any model in 8-bit precision."
      ],
      "metadata": {
        "id": "pdymooqe1pVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: class `W8A16LinearLayer`\n",
        "\n",
        "- Build the target class, `W8A16LinearLayer()`, that will be responsible for quantizing your model."
      ],
      "metadata": {
        "id": "EkYuIWRd1qll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 - `w8_a16_forward` Function\n",
        "\n",
        "-\n",
        "```Python\n",
        "W8A16LinearLayer\n",
        "                    # 8-bit  # 16-bit         # optional\n",
        "* w8_a16_forward -> weights, input,   scales, bias=None\n",
        "                    \n",
        "```\n",
        "- Cast the 8-bit `weights` to the same data type as the `input`, \"casted weights\",\n",
        "- keeping the \"casted weights\" in the same range as before, [-128, 127]\n",
        "- Next, $$(({inputs} \\cdot \\text{``casted weights''}) * {scale}) + {bias}$$"
      ],
      "metadata": {
        "id": "-Y4OWYi-1rxy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuF2-xnP1mVV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_int8 = torch.randint(-128, 127, (32, 16)).to(torch.int8)\n",
        "random_hs = torch.randn((1, 16), dtype=torch.bfloat16)\n",
        "scales = torch.randn((1, 32), dtype=torch.bfloat16)\n",
        "bias = torch.randn((1, 32), dtype=torch.bfloat16)"
      ],
      "metadata": {
        "id": "J8VikG1l1upt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Since the values are random, what you see in the video might be different than what you will get."
      ],
      "metadata": {
        "id": "nmTHdmfw1vhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "F.linear(random_hs, random_int8.to(random_hs.dtype))"
      ],
      "metadata": {
        "id": "cpBBZvtJ1wwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F.linear(random_hs, random_int8.to(random_hs.dtype)) * scales"
      ],
      "metadata": {
        "id": "llHUm0aO1xng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(F.linear(random_hs, random_int8.to(random_hs.dtype)) * scales) + bias"
      ],
      "metadata": {
        "id": "LRRagwJj1zLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Implement all this as a function, `w8_a16_forward`"
      ],
      "metadata": {
        "id": "2keLGcO910GL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def w8_a16_forward(weight, input, scales, bias=None):\n",
        "\n",
        "    casted_weights = weight.to(input.dtype)\n",
        "    output = F.linear(input, casted_weights) * scales\n",
        "\n",
        "    if bias is not None:\n",
        "        output = output + bias\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "JDP2A-OK11Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"With bias:\\n\\n\",\n",
        "      w8_a16_forward(random_int8, random_hs, scales, bias))\n",
        "\n",
        "print(\"\\nWithout bias:\\n\\n\",\n",
        "      w8_a16_forward(random_int8, random_hs, scales))"
      ],
      "metadata": {
        "id": "Shu2lNc413N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 - `init` Function of class `W8A16LinearLayer`"
      ],
      "metadata": {
        "id": "41PDD71p15Z1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This is how the `init` is of [PyTorch Linear layer](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear):\n",
        "\n",
        "```Python\n",
        "def __init__(self, in_features, out_features, bias=True,\n",
        "             device=None, dtype=None)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "mhAhZAR817IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### running this will result in an error\n",
        "class W8A16LinearLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features,\n",
        "                 bias=True, dtype=torch.float32):\n",
        "        super().__init__()\n",
        "\n",
        "        self.int8_weights = nn.Parameter(torch.Tensor([0, 1]\n",
        "                                     ).to(dtype=torch.int8))\n",
        "\n",
        "try:\n",
        "\n",
        "    W8A16LinearLayer(1, 1)\n",
        "\n",
        "except Exception as error:\n",
        "    print(\"\\033[91m\", type(error).__name__, \": \", error, \"\\033[0m\")"
      ],
      "metadata": {
        "id": "1renH-is14iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class W8A16LinearLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features,\n",
        "                 bias=True, dtype=torch.float32):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"int8_weights\",\n",
        "            torch.randint(\n",
        "                -128, 127, (out_features, in_features), dtype=torch.int8\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.register_buffer(\"scales\",\n",
        "                             torch.randn((out_features), dtype=dtype))\n",
        "\n",
        "        if bias:\n",
        "            self.register_buffer(\"bias\",\n",
        "                                 torch.randn((1, out_features),\n",
        "                                             dtype=dtype))\n",
        "\n",
        "        else:\n",
        "            self.bias = None"
      ],
      "metadata": {
        "id": "ZV2jtGBc1-eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Test your implementation."
      ],
      "metadata": {
        "id": "Gt8Vm-ng2BFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_instance = W8A16LinearLayer(16, 32)"
      ],
      "metadata": {
        "id": "7fjzux_Y2Bg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dummy_instance.int8_weights.shape)\n",
        "print(dummy_instance.scales.shape)"
      ],
      "metadata": {
        "id": "-Bsn3SIv2CRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 - `forward` Function of class `W8A16LinearLayer`\n",
        "\n",
        "- Use the `w8_a16_forward` defined earlier (Step 1.1) to define the `forward` function."
      ],
      "metadata": {
        "id": "KtznKUCy2EbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class W8A16LinearLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features,\n",
        "                 bias=True, dtype=torch.float32):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"int8_weights\",\n",
        "            torch.randint(\n",
        "                -128, 127, (out_features, in_features), dtype=torch.int8\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.register_buffer(\"scales\",\n",
        "                             torch.randn((out_features), dtype=dtype))\n",
        "\n",
        "        if bias:\n",
        "            self.register_buffer(\"bias\",\n",
        "                                 torch.randn((1, out_features),\n",
        "                                             dtype=dtype))\n",
        "\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return w8_a16_forward(self.int8_weights,\n",
        "                              input, self.scales, self.bias)"
      ],
      "metadata": {
        "id": "H4Nmd_7L2Etv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module = W8A16LinearLayer(16, 32)\n",
        "dummy_hidden_states = torch.randn(1, 6, 16)"
      ],
      "metadata": {
        "id": "WfchiY0-2G7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module(dummy_hidden_states).shape"
      ],
      "metadata": {
        "id": "6FBCVmIE2Hzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module(dummy_hidden_states).dtype"
      ],
      "metadata": {
        "id": "SjsoqNNG2IuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 - `quantize` Function of class `W8A16LinearLayer`\n",
        "\n",
        "- `quantize` function will dynamically quantize half-precision weights into `torch.int8`"
      ],
      "metadata": {
        "id": "hcMLme5Q2Jq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class W8A16LinearLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features,\n",
        "                 bias=True, dtype=torch.float32):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"int8_weights\",\n",
        "            torch.randint(\n",
        "                -128, 127, (out_features, in_features), dtype=torch.int8\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.register_buffer(\"scales\",\n",
        "                             torch.randn((out_features), dtype=dtype))\n",
        "\n",
        "        if bias:\n",
        "            self.register_buffer(\"bias\",\n",
        "                                 torch.randn((1, out_features),\n",
        "                                             dtype=dtype))\n",
        "\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def quantize(self, weights):\n",
        "        w_fp32 = weights.clone().to(torch.float32)\n",
        "\n",
        "        scales = w_fp32.abs().max(dim=-1).values / 127\n",
        "        scales = scales.to(weights.dtype)\n",
        "\n",
        "        int8_weights = torch.round(weights\n",
        "                        /scales.unsqueeze(1)).to(torch.int8)\n",
        "\n",
        "        self.int8_weights = int8_weights\n",
        "        self.scales = scales\n",
        "\n",
        "    def forward(self, input):\n",
        "        return w8_a16_forward(self.int8_weights,\n",
        "                              input, self.scales, self.bias)"
      ],
      "metadata": {
        "id": "8CXadz2r2KR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module = W8A16LinearLayer(4, 8)"
      ],
      "metadata": {
        "id": "Y52RxBP52NoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Weights before:\\n\" , module.int8_weights)"
      ],
      "metadata": {
        "id": "MXnM4FVX2O0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_matrix = torch.randn((4, 8), dtype=torch.bfloat16)"
      ],
      "metadata": {
        "id": "sVKufF5G2PyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module.quantize(random_matrix)"
      ],
      "metadata": {
        "id": "4to1mXwI2Qmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Weights After:\\n\" , module.int8_weights)"
      ],
      "metadata": {
        "id": "6F1lVC6e2RVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module.scales"
      ],
      "metadata": {
        "id": "n84pX_PJ2SGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module.scales.shape"
      ],
      "metadata": {
        "id": "XjNPn-d72S8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module.int8_weights.shape"
      ],
      "metadata": {
        "id": "1JS8PrdW2UFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### dequantized weights\n",
        "module.int8_weights * module.scales.unsqueeze(1)"
      ],
      "metadata": {
        "id": "aSDkQwv_2VFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### original weights\n",
        "random_matrix"
      ],
      "metadata": {
        "id": "Ea2YmCiv2W3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(random_matrix - module.int8_weights\n",
        " * module.scales.unsqueeze(1)).abs().mean()"
      ],
      "metadata": {
        "id": "9v0Iy-V52YUr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}